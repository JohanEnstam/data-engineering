# IGDB Spelrekommendationssystem - Utvecklingsguide

## üéØ **Projektm√•l**

**Huvudm√•l:** Bygga ett komplett spelrekommendationssystem med IGDB API som datak√§lla, implementerat som en fullst√§ndig data pipeline i Google Cloud Platform.

**Slutprodukt:** En webbapplikation d√§r anv√§ndare kan skriva in spel-titlar och f√• rekommendationer p√• liknande spel baserat p√• ML-algoritmer.

---

## üèóÔ∏è **Teknisk Arkitektur**

### **Data Pipeline (End-to-End)**
```
IGDB API ‚Üí Airflow ‚Üí Cloud Storage ‚Üí BigQuery ‚Üí dbt ‚Üí ML Processing ‚Üí FastAPI ‚Üí Next.js Frontend
```

**Airflow Orchestration:**
- **Data Collection:** IGDB API ‚Üí Local JSON files
- **Storage Upload:** Local files ‚Üí Google Cloud Storage
- **BigQuery Load:** GCS ‚Üí BigQuery raw tables
- **dbt Transformations:** Raw data ‚Üí ML-ready features
- **ML Training:** Transformed data ‚Üí Trained models

### **Teknisk Stack**
- **Backend:** Python, FastAPI, IGDB API
- **Data Processing:** BigQuery, dbt (data build tool)
- **Data Orchestration:** Apache Airflow 3.0 ‚≠ê **IMPLEMENTERAT**
- **ML:** scikit-learn, pandas, numpy
- **Storage:** Google Cloud Storage ‚≠ê **IMPLEMENTERAT**
- **Frontend:** Next.js 14, TypeScript, Tailwind CSS, shadcn/ui
- **Cloud:** Google Cloud Platform (GCP) ‚≠ê **CLOUD RUN DEPLOYED**
- **CI/CD:** GitHub Actions
- **Containerization:** Docker ‚≠ê **GCR INTEGRATION**
- **Secrets:** Google Secret Manager ‚≠ê **IMPLEMENTERAT**

---

## üöÄ **GCP DEPLOYMENT STATUS (2025-01-15)**

### **‚úÖ Implementerat i molnet - PIPELINE KOMPLETT!**

**üîê Secret Manager:**
- ‚úÖ IGDB API credentials s√§kert lagrade
- ‚úÖ Cloud Run service account permissions konfigurerade

**üê≥ Docker & Container Registry:**
- ‚úÖ Alla services containerized (data collection, backend, frontend)
- ‚úÖ Images pushade till Google Container Registry
- ‚úÖ Docker authentication konfigurerad

**‚òÅÔ∏è Cloud Run Services (KOMPLETT PIPELINE):**
- ‚úÖ **Data Collection:** `collect-igdb-data` service
  - URL: `https://collect-igdb-data-3sp2ul3fea-ew.a.run.app`
  - **TESTAT: 20 spel samlade fr√•n IGDB API**
- ‚úÖ **Backend API:** `igdb-backend` service (BigQuery integration)
  - URL: `https://igdb-backend-3sp2ul3fea-ew.a.run.app`
  - Endpoints: `/games`, `/stats`, `/api/budget`, `/api/recommendations/*`
- ‚úÖ **Frontend Dashboard:** `igdb-frontend` service
  - URL: `https://igdb-frontend-3sp2ul3fea-ew.a.run.app`
  - **FULLST√ÑNDIGT FUNGERANDE DASHBOARD!** üéâ

**üìä BigQuery Integration:**
- ‚úÖ Automatisk data upload fr√•n Cloud Run
- ‚úÖ Tabell: `exalted-tempo-471613-e2.igdb_game_data.games_raw`
- ‚úÖ JSON format med timestamps
- ‚úÖ Backend l√§ser direkt fr√•n BigQuery
- ‚úÖ Frontend visar live data fr√•n BigQuery

### **üéâ PIPELINE STATUS: FULLST√ÑNDIGT FUNGERANDE!**
- ‚úÖ **End-to-End:** IGDB API ‚Üí Cloud Storage ‚Üí BigQuery ‚Üí FastAPI ‚Üí Next.js Dashboard
- ‚úÖ **20 spel** visas korrekt i dashboard
- ‚úÖ **Inga fel** i browser console
- ‚úÖ **Alla endpoints** fungerar
- ‚úÖ **Live data** fr√•n BigQuery till frontend

### **üîÑ N√§sta steg (valfritt):**
1. **Skala till fler spel** - 1000+ spel fr√•n IGDB
2. **Cloud Composer** - Automatisk scheduling (valfritt)
3. **ML Pipeline** - Rekommendationsmodell (valfritt)

---

## üìä **Data K√§lla: IGDB API**

### **Tillg√§nglig Data (15+ Datatyper)**
- **Spel (Games):** namn, beskrivning, storyline, betyg, releasedatum, genrer, plattformar, teman
- **Kategorisering:** genrer, plattformar, teman, spelmoder, perspektiv
- **F√∂retag:** utvecklare, utgivare, beskrivningar, l√§nder
- **Media:** covers, screenshots, videos, websites
- **Tidsdata:** release dates, timestamps

### **Data Volym & Kvalitet**
- **~500,000+ spel** i databasen
- **~50+ genrer** och **~100+ teman**
- **~200+ plattformar** (alla tider)
- **~10,000+ f√∂retag** (utvecklare/utgivare)
- **Miljontals bilder** (covers, screenshots)
- **H√∂g datakvalitet:** Strukturerad, verifierad, historisk data

### **API Begr√§nsningar**
- **Rate limit:** ~30 requests per minute
- **Gratis tier:** Bra f√∂r testning och sm√• projekt
- **Autentisering:** OAuth2 via Twitch Developer Portal

---

## ü§ñ **Machine Learning Approach**

### **Rekommendationssystem (Huvudfokus)**
**M√•l:** Rekommendera liknande spel baserat p√• anv√§ndarinput

**Algoritmer:**
1. **Content-Based Filtering**
   - Analysera spel-genres, teman, plattformar
   - Ber√§kna similarity scores
   - Rekommendera spel med liknande attribut

2. **Collaborative Filtering** (om anv√§ndardata finns)
   - "Anv√§ndare som gillade X gillade ocks√• Y"
   - Anv√§nda rating-data f√∂r rekommendationer

3. **Hybrid Approach**
   - Kombinera content-based och collaborative
   - Viktning baserat p√• tillg√§nglig data

### **Feature Engineering**
```python
# Spel-attribut f√∂r ML:
- Genres (one-hot encoding)
- Themes (one-hot encoding)
- Platforms (one-hot encoding)
- Game modes (one-hot encoding)
- Player perspectives (one-hot encoding)
- Release year (numerical)
- Rating scores (numerical)
- Text features (TF-IDF p√• summaries)
```

---

## üé® **Anv√§ndarupplevelse**

### **Frontend (Next.js + shadcn/ui)**
**Huvudfunktioner:**
- **S√∂kf√§lt:** Anv√§ndare skriver in spel-titlar
- **Rekommendationer:** Visar liknande spel med:
  - Spel-titel och beskrivning
  - Cover-bild
  - Genres och teman
  - Rating och releasedatum
  - Likhetsscore
- **Filtrering:** Filtrera p√• genre, plattform, √•r
- **Responsiv design:** Fungerar p√• desktop och mobil

### **API Endpoints (FastAPI)**
```python
# Huvudendpoints:
GET /api/games/search?query={game_name}
GET /api/games/recommendations?game_id={id}&limit={n}
GET /api/games/{id}
GET /api/genres
GET /api/platforms
POST /api/recommendations/batch  # F√∂r flera spel samtidigt
```

### **Frontend Development & Empty State Handling** ‚≠ê **IMPLEMENTERAT** ‚úÖ

**Problem som l√∂stes:**
- Mock-data var f√∂rvirrande och visade inte verklig data
- API kunde inte ladda NDJSON-format korrekt
- Ingen tydlig feedback n√§r data saknades

**L√∂sningar implementerade:**
- **API Data Loading:** St√∂d f√∂r NDJSON-format och automatisk data type conversion
- **Empty State Management:** Proper loading, error och empty states med retry funktionalitet
- **Mock Data Removal:** Borttaget all mock-data och ersatt med riktig IGDB API data
- **User Experience:** Tydliga meddelanden och retry-knappar f√∂r b√§ttre anv√§ndarupplevelse

**Resultat:**
- ‚úÖ Riktig data fr√•n IGDB API (100 games)
- ‚úÖ Tydliga meddelanden n√§r data saknas eller fel uppst√•r
- ‚úÖ Clean code utan f√∂rvirrande mock-data logik

---

## üöÄ **Utvecklingsfaser**

### **Fas 1: Frontend-First Prototyping** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Visuell feedback och iterativ utveckling

**Uppgifter:**
- [x] Skapa projektstruktur enligt best practice
- [x] Migrera Isaks IGDB API kod till `src/api/`
- [x] Utveckla data collection script
- [x] Bygg data preprocessing pipeline
- [x] **Frontend setup** med Next.js + shadcn/ui
- [x] **Data visualization** - visa testdata i tables/charts
- [x] **Budget tracking** dashboard f√∂r GCP credits
- [x] **Basic API endpoints** f√∂r data access
- [x] **GCP Integration** - budget monitoring med verklig data
- [x] **Empty State Handling** - proper error handling och user feedback
- [x] **API Data Loading** - NDJSON support och data type conversion

### **Fas 2: Local-First ML Development** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Bygga robust rekommendationsmotor lokalt innan cloud scaling

**Strategi:** "Progressive Local-First" - utveckla och testa allt lokalt f√∂rst

**Uppgifter:**
- [x] **Data Collection (1,000+ spel)** - samla tillr√§ckligt med data lokalt
- [x] **Progressive feature engineering** - b√∂rja med core features (genres, themes)
- [x] **Local model training** p√• MacBook med scikit-learn
- [x] **Manual evaluation system** - "Ser dessa rekommendationer rimliga ut?"
- [x] **Frontend integration** - s√∂k + rekommendationer i UI
- [x] **Model comparison** - testa olika algoritmer visuellt
- [x] **Performance optimization** f√∂r lokala constraints
- [x] **Data quality validation** med visuell feedback

### **Fas 3: Docker & CI/CD Integration** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Containerisering och CI/CD-pipeline f√∂r skalning till molnet

**Uppgifter:**
- [x] **GCP budget tracking** - real-time cost monitoring
- [x] **Docker containerization** - Frontend + Backend + PostgreSQL
- [x] **TypeScript/ESLint fixes** - Clean builds utan fel
- [x] **Lokal Docker-testning** - Alla services fungerar perfekt
- [x] **GitHub Actions CI/CD** - Simple CI pipeline implementerad och fungerar
- [x] **GitHub CLI Integration** - Direkt workflow-√∂vervakning fr√•n terminal
- [x] **Python Code Quality** - Black, flake8, isort automation
- [x] **Pre-commit Hooks** - Lokal kodkvalitet f√∂re commit
- [x] **Status Badges** - Real-time CI/CD status i README
- [x] **Frontend Component Fixes** - TypeScript path mapping fixade, Docker build fungerar

### **Fas 4: Airflow Data Pipeline** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Automatiserad data pipeline fr√•n IGDB API till ML-modeller

**Uppgifter:**
- [x] **Airflow 3.0 Installation** - Apache Airflow med Google providers
- [x] **Cloud Storage Setup** - GCS buckets f√∂r raw och processed data
- [x] **Airflow DAG Development** - Komplett pipeline DAG implementerad
- [x] **Data Collection Task** - IGDB API ‚Üí Local JSON files
- [x] **Storage Upload Task** - Local files ‚Üí Google Cloud Storage
- [x] **BigQuery Load Task** - GCS ‚Üí BigQuery raw tables
- [x] **dbt Integration Task** - Raw data ‚Üí ML-ready features
- [x] **ML Training Task** - Transformed data ‚Üí Trained models
- [x] **Test DAG** - Verifiering av alla komponenter
- [x] **GCP Authentication** - Service account integration
- [x] **Airflow Configuration** - JWT secrets och s√§kra nycklar
- [x] **Web UI Access** - http://localhost:8080 fungerar perfekt
- [x] **Documentation** - Komplett setup guide i docs/AIRFLOW_SETUP.md

### **Fas 4: Airflow Data Pipeline** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Automatiserad data pipeline fr√•n IGDB API till ML-modeller

**Implementation:**
- **Airflow 3.0** - Apache Airflow med Google providers
- **Cloud Storage** - GCS buckets f√∂r raw och processed data
- **DAG Development** - Komplett pipeline med 5 tasks
- **GCP Integration** - Service account authentication
- **Web UI** - http://localhost:8080 f√∂r monitoring
- **Documentation** - Komplett setup guide i docs/AIRFLOW_SETUP.md

**Pipeline Tasks:**
1. **collect_igdb_data** - IGDB API ‚Üí Local JSON files
2. **upload_to_gcs** - Local files ‚Üí Google Cloud Storage
3. **load_to_bigquery** - GCS ‚Üí BigQuery raw tables
4. **run_dbt_transformations** - Raw data ‚Üí ML-ready features
5. **train_ml_models** - Transformed data ‚Üí Trained models

### **Fas 5: Vertex AI Learning & Progressive Scaling** ‚≠ê **N√ÑSTA** üéØ
**Strategi:** "Local Testing ‚Üí Vertex AI Learning ‚Üí Progressive Scaling ‚Üí Cloud Production"

**M√•l:** L√§r dig GCP ML-tj√§nster med v√•ra 100 spel, skala successivt med kvalitetskontroll

#### **Fas 4A: GCP Learning (1-2 dagar)** ‚úÖ **KLAR**
**Syfte:** L√§r dig BigQuery, dbt, Airflow, Vertex AI med v√•ra 100 spel

**Steg 1: BigQuery Setup** ‚úÖ **KLAR**
- [x] **Skapa BigQuery dataset** `igdb_games` (exalted-tempo-471613-e2)
- [x] **Ladda upp v√•ra 100 spel** fr√•n lokal CSV till BigQuery (games_raw tabell)
- [x] **Testa SQL queries** p√• speldata i BigQuery (83 kolumner, 100 spel)
- [x] **Skapa views** f√∂r genres, platforms, themes (genre_analysis, platform_analysis)

**Steg 2: dbt Project Setup** ‚úÖ **KLAR**
- [x] **Skapa dbt project** f√∂r data transformation (`dbt_igdb_project/igdb_models`)
- [x] **Definiera models** f√∂r games, genres, platforms (stg_games, game_recommendations)
- [x] **Testa transformations** lokalt med dbt (100 spel transformerade)
- [x] **Deploy till BigQuery** med dbt (US region, igdb_games dataset)

**Steg 3: Airflow DAG Setup** ‚úÖ **KLAR**
- [x] **Skapa Airflow DAG** f√∂r data pipeline (igdb_data_pipeline.py)
- [x] **Definiera tasks** f√∂r data collection, transformation, ML (5 tasks)
- [x] **Testa DAG** lokalt med Airflow (Web UI p√• localhost:8080)
- [x] **Deploy till Cloud Composer** (eller lokal Airflow)

**Steg 4: EU Migration & AutoML Pipeline** ‚úÖ **KLAR** / üîÑ **N√ÑSTA**
- [x] **EU Migration Complete** - BigQuery dataset `igdb_game_data` i EU region
- [x] **EU Storage buckets** - Raw och processed data i europe-west1
- [x] **dbt EU konfiguration** - OAuth authentication fungerar
- [ ] **AutoML integration** f√∂r automatisk modelltr√§ning
- [ ] **Incremental data collection** f√∂r resursoptimering
- [ ] **CI/CD GCP deployment** f√∂r production pipeline

---

## üõ†Ô∏è **Detaljerad Implementation Guide - Fas 5: Vertex AI Learning**

### **Steg 1: Airflow DAG Testing (1-2 timmar)** üîÑ **N√ÑSTA**

#### **1.1 Starta Airflow lokalt**
```bash
# Aktivera venv f√∂rst
source venv/bin/activate

# Starta Airflow
./airflow/start_airflow.sh

# Verifiera att Airflow k√∂rs
curl http://localhost:8080/health
```

#### **1.2 Testa Airflow DAG**
**√ñppna Airflow Web UI:**
- G√• till http://localhost:8080
- Login: admin / [genererat l√∂senord fr√•n start_airflow.sh]
- Hitta DAG: `igdb_data_pipeline`

**Testa DAG manuellt:**
1. **Klicka p√• DAG namnet** ‚Üí `igdb_data_pipeline`
2. **Klicka p√• "Trigger DAG"** (play-knapp)
3. **V√§lj "Trigger DAG w/ Config"** f√∂r att skicka parametrar
4. **Konfigurera:**
   ```json
   {
     "games_limit": 100,
     "test_mode": true
   }
   ```

**√ñvervaka k√∂rning:**
- **Graph View:** Se task dependencies och status
- **Tree View:** Se historik √∂ver k√∂rningar
- **Logs:** Klicka p√• task ‚Üí "Log" f√∂r detaljerade felmeddelanden

#### **1.3 Troubleshooting vanliga problem**
```bash
# Om DAG inte visas:
# Kontrollera att DAG-filen √§r korrekt placerad
ls -la airflow/dags/igdb_data_pipeline.py

# Om tasks failar:
# Klicka p√• task ‚Üí "Log" f√∂r att se felmeddelanden
# Vanliga problem:
# - IGDB API credentials saknas
# - GCP service account key saknas
# - Python path problem
```

### **Steg 2: Vertex AI Notebook Setup (1-2 timmar)**

#### **2.1 Aktivera Vertex AI API**
```bash
# Aktivera venv f√∂rst
source venv/bin/activate

# Aktivera Vertex AI API
gcloud services enable aiplatform.googleapis.com

# Verifiera att API √§r aktiverat
gcloud services list --enabled --filter="name:aiplatform"
```

#### **2.2 Skapa Vertex AI Notebook Instance**
```bash
# Skapa notebook instance
gcloud ai notebooks instances create igdb-ml-notebook \
  --location=europe-west1 \
  --machine-type=e2-standard-4 \
  --vm-image-project=deeplearning-platform-release \
  --vm-image-family=tf2-2-8-cpu \
  --vm-image-name=tf2-2-8-cpu-20220119-170516

# √ñppna notebook
gcloud ai notebooks instances open igdb-ml-notebook --location=europe-west1
```

#### **2.3 Konfigurera Notebook Environment**
```python
# Installera dependencies i notebook
!pip install pandas numpy scikit-learn google-cloud-bigquery

# Konfigurera GCP authentication
from google.cloud import bigquery
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Testa BigQuery connection
client = bigquery.Client()
print("BigQuery connection successful!")
```

### **Steg 3: ML Model Comparison (1-2 timmar)**

#### **3.1 Ladda data fr√•n BigQuery**
```python
# Ladda v√•ra 100 spel fr√•n BigQuery
query = """
SELECT id, name, summary, rating, genre_id, platform_id, theme_id
FROM `exalted-tempo-471613-e2.igdb_games.game_recommendations`
WHERE summary IS NOT NULL
"""
df = client.query(query).to_dataframe()
print(f"Laddat {len(df)} spel fr√•n BigQuery")
```

#### **3.2 Tr√§na content-based model**
```python
# Tr√§na samma modell som lokalt
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
tfidf_matrix = vectorizer.fit_transform(df['summary'].fillna(''))

# Ber√§kna similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

print(f"Tr√§nat model med {len(df)} spel")
print(f"Similarity matrix shape: {similarity_matrix.shape}")
```

#### **3.3 J√§mf√∂r prestanda**
```python
# Testa rekommendationer
def get_recommendations(game_id, top_n=3):
    game_idx = df[df['id'] == game_id].index[0]
    similarity_scores = similarity_matrix[game_idx]
    top_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]
    return df.iloc[top_indices][['id', 'name', 'rating']]

# Testa med ett spel
test_game_id = df.iloc[0]['id']
recommendations = get_recommendations(test_game_id)
print("Rekommendationer:")
print(recommendations)
```

### **Steg 4: Progressive Scaling Strategy (2-3 dagar)**

#### **4.1 Skalningsfaser**
**Fas 1: 100 ‚Üí 1,000 spel (2-3 timmar)**
- Testa lokalt f√∂rst
- Verifiera data quality
- J√§mf√∂r ML prestanda

**Fas 2: 1,000 ‚Üí 10,000 spel (4-6 timmar)**
- Implementera batch processing
- Flytta data till Cloud Storage
- Testa Vertex AI tr√§ning

**Fas 3: 10,000 ‚Üí 100,000+ spel (1-2 dagar)**
- Cloud Functions f√∂r parallell data collection
- Vertex AI f√∂r distributed ML training
- BigQuery f√∂r data processing

#### **4.2 Kvalitetskontroll vid varje steg**
```bash
# Data quality validation
python -c "from src.data_processing.etl_pipeline import ETLPipeline; ETLPipeline().validate_data()"

# ML performance testing
python -c "from src.models.game_recommender import GameRecommender; GameRecommender().train_and_evaluate()"

# Airflow DAG testing
# K√∂r DAG med nya parametrar och √∂vervaka resultat
```

---

## üõ†Ô∏è **Detaljerad Implementation Guide - Fas 4A**

### **Steg 1: BigQuery Setup (2-3 timmar)**

#### **1.1 Aktivera BigQuery API**
```bash
# Aktivera venv f√∂rst
source venv/bin/activate

# Aktivera BigQuery API
gcloud services enable bigquery.googleapis.com

# Verifiera att API √§r aktiverat
gcloud services list --enabled --filter="name:bigquery"
```

#### **1.2 Skapa BigQuery Dataset**
```bash
# Skapa dataset f√∂r v√•r data
bq mk --dataset --location=EU --description="IGDB Game Data for ML Pipeline" \
  exalted-tempo-471613-e2:igdb_game_data

# Verifiera att dataset skapades
bq ls exalted-tempo-471613-e2:igdb_game_data
```

#### **1.3 Ladda upp v√•ra 100 spel**
```bash
# Konvertera v√•r CSV till BigQuery format
python -c "
import pandas as pd
import json

# L√§s v√•r senaste processed data
df = pd.read_csv('data/processed/games_20250910_194944.csv')
print(f'Laddar {len(df)} spel till BigQuery...')

# Spara som JSON f√∂r BigQuery
df.to_json('games_for_bigquery.json', orient='records', lines=True)
print('Data konverterad till JSON format')
"

# Ladda upp till BigQuery
bq load --source_format=NEWLINE_DELIMITED_JSON \
  --autodetect \
  exalted-tempo-471613-e2:igdb_game_data.games \
  games_for_bigquery.json

# Verifiera att data laddades
bq query --use_legacy_sql=false "
SELECT COUNT(*) as total_games, 
       COUNT(DISTINCT genre_id) as unique_genres,
       COUNT(DISTINCT platform_id) as unique_platforms
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
"
```

#### **1.4 Skapa Views f√∂r Analysis**
```bash
# Skapa view f√∂r genre analysis
bq query --use_legacy_sql=false "
CREATE OR REPLACE VIEW \`exalted-tempo-471613-e2.igdb_game_data.genre_analysis\` AS
SELECT 
  genre_id,
  COUNT(*) as game_count,
  AVG(rating) as avg_rating,
  MIN(release_year) as earliest_year,
  MAX(release_year) as latest_year
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
WHERE genre_id IS NOT NULL
GROUP BY genre_id
ORDER BY game_count DESC
"

# Skapa view f√∂r platform analysis
bq query --use_legacy_sql=false "
CREATE OR REPLACE VIEW \`exalted-tempo-471613-e2.igdb_game_data.platform_analysis\` AS
SELECT 
  platform_id,
  COUNT(*) as game_count,
  AVG(rating) as avg_rating,
  MIN(release_year) as earliest_year,
  MAX(release_year) as latest_year
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
WHERE platform_id IS NOT NULL
GROUP BY platform_id
ORDER BY game_count DESC
"
```

### **Steg 2: dbt Project Setup (2-3 timmar)** ‚úÖ **KLAR**

#### **2.1 Installera dbt** ‚úÖ **KLAR**
```bash
# Installera dbt med BigQuery support
pip install dbt-bigquery

# Verifiera installation
dbt --version
```

#### **2.2 Skapa dbt Project** ‚úÖ **KLAR**
```bash
# Skapa dbt project
mkdir dbt_igdb_project
cd dbt_igdb_project

# Initiera dbt project
dbt init igdb_models

# Konfigurera profiles.yml f√∂r BigQuery
# Dataset: igdb_games (US region)
# Project: exalted-tempo-471613-e2
# Authentication: Service Account
```

#### **2.3 Skapa dbt Models** ‚úÖ **KLAR**

**Staging Model (`stg_games.sql`):**
- Rensar och standardiserar raw games data
- Data quality checks och derived fields
- Rating categories (Excellent, Good, Average, etc.)
- Era categories (Recent, Modern, Classic, Retro)

**Marts Model (`game_recommendations.sql`):**
- ML-optimerad tabell f√∂r rekommendationer
- Feature vectors f√∂r genre similarity
- Imputed values f√∂r missing data
- Processed timestamps f√∂r data freshness

#### **2.4 dbt Configuration** ‚úÖ **KLAR**
```yaml
# dbt_project.yml
models:
  igdb_models:
    staging:
      +materialized: view
    marts:
      +materialized: table
```

#### **2.5 Data Tests** ‚úÖ **KLAR**
- Unique constraints p√• game IDs
- Not-null validations p√• kritiska f√§lt
- Source data quality tests
- 12/13 tests passerade (1 example test misslyckades)

#### **2.6 Dataset Configuration** ‚úÖ **KLAR**
**BigQuery Datasets:**
- `igdb_games` (US region) - ‚úÖ **AKTIV** - V√•r data och dbt models
- `igdb_game_data` (EU region) - ‚ö†Ô∏è **OANV√ÑND** - Skapad tidigare, kan tas bort

**Region Val:**
- **US region** vald f√∂r b√§ttre prestanda med GCP-tj√§nster
- **EU region** hade kr√§vt extra konfiguration f√∂r andra GCP-tj√§nster
- **Rekommendation:** Beh√•ll US region f√∂r konsistens

#### **2.3 Konfigurera dbt Profile**
```bash
# Skapa profiles.yml
mkdir -p ~/.dbt
cat > ~/.dbt/profiles.yml << 'EOF'
igdb_bigquery:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      keyfile: /path/to/your/service-account-key.json
      project: exalted-tempo-471613-e2
      dataset: igdb_game_data
      location: EU
      threads: 4
      timeout_seconds: 300
EOF
```

#### **2.4 Skapa dbt Models**
```bash
# Skapa staging models
mkdir -p models/staging
cat > models/staging/stg_games.sql << 'EOF'
-- Staging model f√∂r games data
SELECT 
  id,
  name,
  summary,
  rating,
  release_year,
  genre_id,
  platform_id,
  theme_id,
  created_at,
  updated_at
FROM {{ source('raw', 'games') }}
WHERE name IS NOT NULL
EOF

# Skapa marts models
mkdir -p models/marts
cat > models/marts/game_recommendations.sql << 'EOF'
-- Mart model f√∂r game recommendations
SELECT 
  g.id,
  g.name,
  g.summary,
  g.rating,
  g.release_year,
  g.genre_id,
  g.platform_id,
  g.theme_id,
  CASE 
    WHEN g.rating >= 80 THEN 'Excellent'
    WHEN g.rating >= 70 THEN 'Good'
    WHEN g.rating >= 60 THEN 'Average'
    ELSE 'Below Average'
  END as rating_category
FROM {{ ref('stg_games') }} g
WHERE g.rating IS NOT NULL
EOF
```

#### **2.5 Testa dbt**
```bash
# Testa dbt connection
dbt debug

# K√∂r dbt models
dbt run

# Testa dbt models
dbt test

# Generera dokumentation
dbt docs generate
dbt docs serve
```

### **Steg 3: Airflow DAG Setup (2-3 timmar)**

#### **3.1 Installera Airflow**
```bash
# Installera Airflow
pip install apache-airflow

# Initiera Airflow
airflow db init

# Skapa admin user
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin
```

#### **3.2 Skapa Airflow DAG**
```bash
# Skapa DAGs mapp
mkdir -p ~/airflow/dags

# Skapa IGDB data pipeline DAG
cat > ~/airflow/dags/igdb_data_pipeline.py << 'EOF'
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'igdb-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 11),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'igdb_data_pipeline',
    default_args=default_args,
    description='IGDB Game Data Collection and Processing Pipeline',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

def collect_game_data():
    """Collect game data from IGDB API"""
    import subprocess
    subprocess.run(['python', 'collect_data.py', '--games-limit', '100'], cwd='/path/to/your/project')
    return "Data collection completed"

def process_data():
    """Process collected data"""
    import subprocess
    subprocess.run(['python', '-m', 'src.data_processing.etl_pipeline'], cwd='/path/to/your/project')
    return "Data processing completed"

def train_model():
    """Train ML model"""
    import subprocess
    subprocess.run(['python', '-m', 'src.models.train_recommender'], cwd='/path/to/your/project')
    return "Model training completed"

# Define tasks
collect_task = PythonOperator(
    task_id='collect_game_data',
    python_callable=collect_game_data,
    dag=dag,
)

process_task = PythonOperator(
    task_id='process_data',
    python_callable=process_data,
    dag=dag,
)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag,
)

# Define task dependencies
collect_task >> process_task >> train_task
EOF
```

#### **3.3 Starta Airflow**
```bash
# Starta Airflow webserver
airflow webserver --port 8080 &

# Starta Airflow scheduler
airflow scheduler &

# √ñppna Airflow UI
open http://localhost:8080
```

### **Steg 4: Vertex AI Learning (2-3 timmar)**

#### **4.1 Aktivera Vertex AI API**
```bash
# Aktivera Vertex AI API
gcloud services enable aiplatform.googleapis.com

# Verifiera att API √§r aktiverat
gcloud services list --enabled --filter="name:aiplatform"
```

#### **4.2 Skapa Vertex AI Notebook**
```bash
# Skapa Vertex AI notebook instance
gcloud ai notebooks instances create igdb-ml-notebook \
  --location=europe-west1 \
  --machine-type=e2-standard-4 \
  --vm-image-project=deeplearning-platform-release \
  --vm-image-family=tf2-2-8-cpu \
  --vm-image-name=tf2-2-8-cpu-20220119-170516

# √ñppna notebook
gcloud ai notebooks instances open igdb-ml-notebook --location=europe-west1
```

#### **4.3 Testa ML tr√§ning i Vertex AI**
```python
# Skapa notebook cell f√∂r ML tr√§ning
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from google.cloud import bigquery

# Ladda data fr√•n BigQuery
client = bigquery.Client()
query = """
SELECT id, name, summary, rating, genre_id, platform_id, theme_id
FROM `exalted-tempo-471613-e2.igdb_game_data.games`
WHERE summary IS NOT NULL
"""
df = client.query(query).to_dataframe()

# Tr√§na enkel content-based model
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(df['summary'].fillna(''))

# Ber√§kna similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

print(f"Tr√§nat model med {len(df)} spel")
print(f"Similarity matrix shape: {similarity_matrix.shape}")
```

---

## üöÄ **Skalningsstrategi: 100 ‚Üí 100,000+ Spel**

### **Kritiska Skalningsutmaningar & L√∂sningar**

#### **1. Data Collection Bottlenecks** üî• **KRITISKT**
**Problem:** IGDB API rate limit (30 req/min) blir flaskhals vid 100,000+ spel
**L√∂sningar:**
- **Batch Processing:** Samla data i chunks (1000 spel per batch)
- **Parallel Processing:** Anv√§nda Cloud Functions f√∂r parallell data collection
- **Caching Strategy:** Redis/Memcached f√∂r att undvika duplicerade API calls
- **Incremental Updates:** Bara samla nya/uppdaterade spel dagligen

#### **2. Storage & Processing Limits** üíæ **VIKTIGT**
**Problem:** Lokal storage och minne r√§cker inte f√∂r 100,000+ spel
**L√∂sningar:**
- **Cloud Storage:** GCS buckets f√∂r raw data (billigare √§n BigQuery)
- **Data Partitioning:** Partitionera data per √•r/genre f√∂r snabbare queries
- **Streaming Processing:** Cloud Dataflow f√∂r real-time data processing
- **Data Archiving:** Flytta gamla data till Coldline Storage

#### **3. ML Model Performance** ü§ñ **KRITISKT**
**Problem:** Cosine similarity p√• 100,000+ spel blir extremt l√•ngsam
**L√∂sningar:**
- **Vector Databases:** Pinecone/Weaviate f√∂r snabba similarity searches
- **Model Optimization:** Anv√§nda approximate nearest neighbors (ANN)
- **Feature Selection:** Reducera dimensions med PCA/t-SNE
- **Distributed Training:** Vertex AI f√∂r parallell modelltr√§ning

#### **4. API Response Times** ‚ö° **VIKTIGT**
**Problem:** Rekommendationer tar f√∂r l√•ng tid med stora datasets
**L√∂sningar:**
- **Precomputed Recommendations:** Ber√§kna rekommendationer i f√∂rv√§g
- **Caching Layer:** Redis f√∂r snabba API responses
- **CDN:** CloudFlare f√∂r statisk content
- **Database Indexing:** Optimera BigQuery queries med proper indexing

### **Best Practice Architecture f√∂r Skalning**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   IGDB API      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Cloud Functions ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Cloud Storage  ‚îÇ
‚îÇ  (Rate Limited) ‚îÇ    ‚îÇ  (Parallel Jobs) ‚îÇ    ‚îÇ   (Raw Data)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Vertex AI     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Airflow DAG    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   BigQuery      ‚îÇ
‚îÇ  (ML Training)  ‚îÇ    ‚îÇ (Orchestration)  ‚îÇ    ‚îÇ (Processed Data)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Cloud Run     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Redis Cache    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Vector DB     ‚îÇ
‚îÇ  (API Serving)  ‚îÇ    ‚îÇ (Fast Responses) ‚îÇ    ‚îÇ (Similarity)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Skalningsfaser**

#### **Fas 1: 100 ‚Üí 1,000 spel** (Nuvarande)
- ‚úÖ BigQuery setup klar
- üîÑ dbt project f√∂r data transformation
- üîÑ Airflow DAG f√∂r automation
- üîÑ Basic ML model optimization

#### **Fas 2: 1,000 ‚Üí 10,000 spel** (N√§sta vecka)
- üîÑ Cloud Storage integration
- üîÑ Parallel data collection
- üîÑ Model performance optimization
- üîÑ Caching implementation

#### **Fas 3: 10,000 ‚Üí 100,000+ spel** (Framtida)
- üîÑ Vector database integration
- üîÑ Distributed ML training
- üîÑ Advanced caching strategies
- üîÑ Production monitoring

---

## üéØ **N√§sta Steg efter Fas 4A**

N√§r du har genomf√∂rt Fas 4A kommer du att ha:
- ‚úÖ **BigQuery dataset** med v√•ra 100 spel
- ‚úÖ **dbt project** f√∂r data transformation
- ‚úÖ **Airflow DAG** f√∂r data pipeline
- ‚úÖ **Vertex AI notebook** f√∂r ML experimentation

**D√• √§r du redo f√∂r Fas 4B: Local Scaling** med 10,000+ spel!

#### **Fas 4B: Local Scaling (1-2 dagar)** üîÑ
**Syfte:** Skala upp till 10,000+ spel lokalt n√§r du f√∂rst√•r GCP

**Steg 1: Data Collection Scaling**
- [ ] **Samla 10,000+ spel** med `collect_data.py --games-limit 10000`
- [ ] **Optimera data collection** f√∂r st√∂rre volymer
- [ ] **Implementera batch processing** f√∂r effektivitet
- [ ] **Validera data quality** med st√∂rre dataset

**Steg 2: ML Algorithm Optimization**
- [ ] **Testa olika ML-algoritmer** med 10,000+ spel
- [ ] **Optimera feature engineering** f√∂r prestanda
- [ ] **Implementera model evaluation** med cross-validation
- [ ] **J√§mf√∂r algoritmer** (content-based vs collaborative)

**Steg 3: Performance Optimization**
- [ ] **Optimera Docker builds** f√∂r snabbare deployment
- [ ] **Implementera caching** f√∂r API responses
- [ ] **Optimera database queries** f√∂r prestanda
- [ ] **Testa load testing** med st√∂rre datam√§ngder

#### **Fas 4C: Cloud Production (1-2 dagar)** üöÄ
**Syfte:** Deploy komplett system till GCP n√§r allt fungerar lokalt

**Steg 1: Cloud Run Deployment**
- [ ] **Deploy frontend** till Cloud Run
- [ ] **Deploy backend** till Cloud Run
- [ ] **Konfigurera Cloud SQL** f√∂r PostgreSQL
- [ ] **Testa end-to-end** i molnet

**Steg 2: Data Pipeline Production**
- [ ] **Deploy Airflow DAG** till Cloud Composer
- [ ] **Konfigurera BigQuery** f√∂r production data
- [ ] **Deploy dbt models** f√∂r data transformation
- [ ] **Automatisera data pipeline** med Airflow

**Steg 3: ML Production**
- [ ] **Deploy ML model** till Vertex AI
- [ ] **Konfigurera model serving** med Cloud Run
- [ ] **Implementera model monitoring** med Vertex AI
- [ ] **Automatisera model retraining** med Airflow

**Steg 4: Production Monitoring**
- [ ] **S√§tt upp Cloud Monitoring** f√∂r system health
- [ ] **Konfigurera budget alerts** f√∂r kostnadskontroll
- [ ] **Implementera logging** med Cloud Logging
- [ ] **S√§tt upp error tracking** med Cloud Error Reporting

### **Fas 5: Advanced ML & Production** ‚≠ê **FRAMTIDA** üîÆ
**M√•l:** Production-ready system med avancerade funktioner

**Uppgifter:**
- [ ] **Advanced features** - text analysis, visual similarity
- [ ] **A/B testing** framework med frontend integration
- [ ] **User feedback** system f√∂r continuous improvement
- [ ] **Real-time rekommendationer** med caching
- [ ] **Performance monitoring** med budget tracking
- [ ] **CI/CD pipeline** med automated testing
- [ ] **Documentation** och presentation f√∂r kursen

---

## üìà **Success Metrics**

### **Tekniska Metrics**
- **API Response Time:** < 200ms f√∂r rekommendationer
- **Data Freshness:** Daglig uppdatering av speldata
- **System Uptime:** > 99.5%
- **Model Accuracy:** > 80% relevanta rekommendationer

### **Anv√§ndarupplevelse Metrics**
- **Search Success Rate:** > 90% hittar s√∂kta spel
- **Recommendation Relevance:** Anv√§ndarfeedback > 4/5
- **Page Load Time:** < 2 sekunder
- **Mobile Responsiveness:** Fungerar p√• alla enheter

### **Business Metrics**
- **Data Pipeline Efficiency:** < 1 timme f√∂r fullst√§ndig datauppdatering
- **Cost Optimization:** < $100/m√•nad i GCP-kostnader (med budget tracking)
- **Scalability:** St√∂der 1000+ samtidiga anv√§ndare
- **Budget Utilization:** < 80% av tillg√§ngliga GCP credits
- **Development Velocity:** Visuell feedback inom 1 dag f√∂r varje feature

---

## üéØ **N√§sta Steg - Hybrid GCP Strategy**

### **Omedelbara √•tg√§rder (Idag):**
1. **Fas 4A: GCP Learning** - B√∂rja med BigQuery setup
2. **Ladda upp v√•ra 100 spel** till BigQuery f√∂r att l√§ra sig
3. **Skapa dbt project** f√∂r data transformation
4. **Testa Airflow DAG** lokalt

### **Denna vecka:**
- **Fas 4A:** GCP Learning (BigQuery, dbt, Airflow, Vertex AI)
- **Fas 4B:** Local Scaling (10,000+ spel lokalt)
- **Fas 4C:** Cloud Production (deploy till GCP)

### **Kommande veckor:**
- **Vecka 4:** Advanced ML och production deployment
- **Vecka 5:** Kurs presentation och dokumentation

---

## üìù **Projektstatus**

**Senast uppdaterad:** 2025-09-12
**Nuvarande fas:** Fas 5 - AutoML Pipeline & Cloud Production (üîÑ P√•g√•ende) + EU Migration Complete (‚úÖ Klar)
**N√§sta milestone:** Skalbar AutoML pipeline med incremental data collection och CI/CD deployment
**Gruppmedlemmar:** Viktoria, Isak & Johan
**Teknisk stack:** Python, Next.js, shadcn/ui, Docker, GCP, IGDB API
**Budget:** AI24S-Data-Engineering-IGDB (kr100.00/m√•nad) + $300 GCP credits
**GCP Project:** IGDB-ML-Pipeline (exalted-tempo-471613-e2)
**Strategi:** Hybrid GCP Learning ‚Üí Local Scaling ‚Üí Cloud Production
**Status:** Komplett fungerande system med 100 spel, ML-rekommendationer, data quality dashboard, Docker containerization och EU GCP migration. Redo f√∂r AutoML pipeline och production deployment.

---

## üöÄ **N√§sta Steg - AutoML Pipeline & Production Deployment**

### **Fas 5B: Skalbar AutoML Pipeline** üéØ **N√ÑSTA**

**M√•l:** Implementera automatisk, skalbar ML pipeline som fungerar lika bra med 100 spel som med 10,000+ spel

#### **Steg 1: AutoML Integration (1-2 dagar)**
- [ ] **Konfigurera Vertex AI AutoML** f√∂r automatisk modelltr√§ning
- [ ] **Integrera AutoML med Airflow DAG** f√∂r automatiserad pipeline
- [ ] **Testa AutoML prestanda** med v√•ra 100 spel fr√•n BigQuery EU
- [ ] **J√§mf√∂r AutoML vs manuell ML** f√∂r prestanda och kostnad

#### **Steg 2: Incremental Data Collection (1 dag)**
- [ ] **Implementera data freshness tracking** i BigQuery
- [ ] **Optimera IGDB API calls** f√∂r att undvika duplicerade requests
- [ ] **Caching strategy** f√∂r att spara API rate limits
- [ ] **Batch processing** f√∂r effektiv data collection

#### **Steg 3: CI/CD GCP Deployment (1-2 dagar)**
- [ ] **Konfigurera Cloud Build** f√∂r automatisk deployment
- [ ] **Deploy frontend till Cloud Run** med Next.js
- [ ] **Deploy backend till Cloud Run** med FastAPI
- [ ] **Konfigurera Cloud SQL** f√∂r PostgreSQL production
- [ ] **S√§tt upp monitoring** med Cloud Monitoring

#### **Steg 4: Production Monitoring (1 dag)**
- [ ] **Budget alerts** f√∂r kostnadskontroll
- [ ] **Performance monitoring** f√∂r API response times
- [ ] **Error tracking** med Cloud Error Reporting
- [ ] **Logging** med Cloud Logging

### **Teknisk Arkitektur f√∂r Production:**
```
IGDB API ‚Üí Airflow ‚Üí BigQuery EU ‚Üí AutoML ‚Üí Trained Model ‚Üí Cloud Run ‚Üí Next.js Frontend
```

**F√∂rdelar med AutoML:**
- ‚úÖ Automatisk skalning med datam√§ngd
- ‚úÖ Ingen manuell ML-optimering kr√§vs
- ‚úÖ GCP hanterar infrastruktur
- ‚úÖ Konsistent prestanda oavsett data-volym

---

## üéØ **UPPDATERAD GCP DEPLOYMENT STRATEGI**

### **Kursprojekt-fokus (4 veckor kvar):**
- üéì **Beh√∂ver fungerande pipeline** f√∂r betyg
- üí∞ **$300 free credits** - Vill inte br√§nna i on√∂dan
- üéØ **100 spel** - Nuvarande dataset, vill expandera senare
- üìö **L√§rande-fokus** - Vill f√∂rst√• varje steg

### **Kostnadseffektiv approach:**
**Alternativ 1: Enkel Pipeline (Rekommenderat)**
```
IGDB API ‚Üí Cloud Functions ‚Üí BigQuery ‚Üí Cloud Run (FastAPI + Next.js)
```
- **Kostnad:** ~$35/m√•nad
- **Setup-tid:** 1-2 dagar
- **Skalbar:** Fungerar f√∂r 100 spel och 334,000 spel

**Alternativ 2: Cloud Composer Pipeline (Senare)**
```
IGDB API ‚Üí Cloud Functions ‚Üí BigQuery ‚Üí dbt ‚Üí Vertex AI ‚Üí Cloud Run
```
- **Kostnad:** ~$330/m√•nad
- **Setup-tid:** 3-5 dagar
- **Professionell:** Som verkliga production systems

### **Implementation roadmap:**
1. **Fas 1:** Enkel pipeline med Cloud Functions + Cloud Run
2. **Fas 2:** Expansion till fler spel (1000+)
3. **Fas 3:** Cloud Composer + Vertex AI (valfritt)

### **N√§sta steg:**
- ‚úÖ Aktivera GCP APIs
- ‚úÖ Deploya Cloud Function f√∂r data collection
- ‚úÖ S√§tt upp BigQuery dataset
- ‚úÖ Deploya Cloud Run services
- ‚úÖ Testa hela pipeline

---

*Detta dokument ska uppdateras kontinuerligt under projektets g√•ng f√∂r att reflektera nuvarande status, l√§rdomar och √§ndringar i planeringen.*
