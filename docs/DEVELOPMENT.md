# IGDB Spelrekommendationssystem - Utvecklingsguide

## ğŸ¯ **ProjektmÃ¥l**

**HuvudmÃ¥l:** Bygga ett komplett spelrekommendationssystem med IGDB API som datakÃ¤lla, implementerat som en fullstÃ¤ndig data pipeline i Google Cloud Platform.

**Slutprodukt:** En webbapplikation dÃ¤r anvÃ¤ndare kan skriva in spel-titlar och fÃ¥ rekommendationer pÃ¥ liknande spel baserat pÃ¥ ML-algoritmer.

---

## ğŸ—ï¸ **Teknisk Arkitektur**

### **Data Pipeline (End-to-End)**
```
IGDB API â†’ Airflow â†’ Cloud Storage â†’ BigQuery â†’ dbt â†’ ML Processing â†’ FastAPI â†’ Next.js Frontend
```

**Airflow Orchestration:**
- **Data Collection:** IGDB API â†’ Local JSON files
- **Storage Upload:** Local files â†’ Google Cloud Storage
- **BigQuery Load:** GCS â†’ BigQuery raw tables
- **dbt Transformations:** Raw data â†’ ML-ready features
- **ML Training:** Transformed data â†’ Trained models

### **Teknisk Stack**
- **Backend:** Python, FastAPI, IGDB API
- **Data Processing:** BigQuery, dbt (data build tool)
- **Data Orchestration:** Apache Airflow 3.0 â­ **IMPLEMENTERAT**
- **ML:** scikit-learn, pandas, numpy
- **Storage:** Google Cloud Storage â­ **IMPLEMENTERAT**
- **Frontend:** Next.js 14, TypeScript, Tailwind CSS, shadcn/ui
- **Cloud:** Google Cloud Platform (GCP) â­ **CLOUD RUN DEPLOYED**
- **CI/CD:** GitHub Actions
- **Containerization:** Docker â­ **GCR INTEGRATION**
- **Secrets:** Google Secret Manager â­ **IMPLEMENTERAT**

---

## ğŸš€ **GCP DEPLOYMENT STATUS (2025-01-15)**

### **âœ… Implementerat i molnet - PIPELINE KOMPLETT!**

**ğŸ” Secret Manager:**
- âœ… IGDB API credentials sÃ¤kert lagrade
- âœ… Cloud Run service account permissions konfigurerade

**ğŸ³ Docker & Container Registry:**
- âœ… Alla services containerized (data collection, backend, frontend)
- âœ… Images pushade till Google Container Registry
- âœ… Docker authentication konfigurerad

**â˜ï¸ Cloud Run Services (KOMPLETT PIPELINE):**
- âœ… **Data Collection:** `collect-igdb-data` service
  - URL: `https://collect-igdb-data-3sp2ul3fea-ew.a.run.app`
  - **TESTAT: 20 spel samlade frÃ¥n IGDB API**
- âœ… **Backend API:** `igdb-backend` service (BigQuery integration)
  - URL: `https://igdb-backend-3sp2ul3fea-ew.a.run.app`
  - Endpoints: `/games`, `/stats`, `/api/budget`, `/api/recommendations/*`
- âœ… **Frontend Dashboard:** `igdb-frontend` service
  - URL: `https://igdb-frontend-3sp2ul3fea-ew.a.run.app`
  - **FULLSTÃ„NDIGT FUNGERANDE DASHBOARD!** ğŸ‰

**ğŸ“Š BigQuery Integration:**
- âœ… Automatisk data upload frÃ¥n Cloud Run
- âœ… Tabell: `exalted-tempo-471613-e2.igdb_game_data.games_raw`
- âœ… JSON format med timestamps
- âœ… Backend lÃ¤ser direkt frÃ¥n BigQuery
- âœ… Frontend visar live data frÃ¥n BigQuery

### **ğŸ‰ PIPELINE STATUS: FULLSTÃ„NDIGT FUNGERANDE!**
- âœ… **End-to-End:** IGDB API â†’ Cloud Storage â†’ BigQuery â†’ FastAPI â†’ Next.js Dashboard
- âœ… **20 spel** visas korrekt i dashboard
- âœ… **Inga fel** i browser console
- âœ… **Alla endpoints** fungerar
- âœ… **Live data** frÃ¥n BigQuery till frontend

### **ğŸ”„ NÃ¤sta steg (valfritt):**
1. **Skala till fler spel** - 1000+ spel frÃ¥n IGDB
2. **Cloud Composer** - Automatisk scheduling (valfritt)
3. **ML Pipeline** - Rekommendationsmodell (valfritt)

---

## ğŸ“Š **Data KÃ¤lla: IGDB API**

### **TillgÃ¤nglig Data (15+ Datatyper)**
- **Spel (Games):** namn, beskrivning, storyline, betyg, releasedatum, genrer, plattformar, teman
- **Kategorisering:** genrer, plattformar, teman, spelmoder, perspektiv
- **FÃ¶retag:** utvecklare, utgivare, beskrivningar, lÃ¤nder
- **Media:** covers, screenshots, videos, websites
- **Tidsdata:** release dates, timestamps

### **Data Volym & Kvalitet**
- **~500,000+ spel** i databasen
- **~50+ genrer** och **~100+ teman**
- **~200+ plattformar** (alla tider)
- **~10,000+ fÃ¶retag** (utvecklare/utgivare)
- **Miljontals bilder** (covers, screenshots)
- **HÃ¶g datakvalitet:** Strukturerad, verifierad, historisk data

### **API BegrÃ¤nsningar**
- **Rate limit:** ~30 requests per minute
- **Gratis tier:** Bra fÃ¶r testning och smÃ¥ projekt
- **Autentisering:** OAuth2 via Twitch Developer Portal

---

## ğŸ¤– **Machine Learning Approach**

### **Rekommendationssystem (Huvudfokus)**
**MÃ¥l:** Rekommendera liknande spel baserat pÃ¥ anvÃ¤ndarinput

**Algoritmer:**
1. **Content-Based Filtering**
   - Analysera spel-genres, teman, plattformar
   - BerÃ¤kna similarity scores
   - Rekommendera spel med liknande attribut

2. **Collaborative Filtering** (om anvÃ¤ndardata finns)
   - "AnvÃ¤ndare som gillade X gillade ocksÃ¥ Y"
   - AnvÃ¤nda rating-data fÃ¶r rekommendationer

3. **Hybrid Approach**
   - Kombinera content-based och collaborative
   - Viktning baserat pÃ¥ tillgÃ¤nglig data

### **Feature Engineering**
```python
# Spel-attribut fÃ¶r ML:
- Genres (one-hot encoding)
- Themes (one-hot encoding)
- Platforms (one-hot encoding)
- Game modes (one-hot encoding)
- Player perspectives (one-hot encoding)
- Release year (numerical)
- Rating scores (numerical)
- Text features (TF-IDF pÃ¥ summaries)
```

---

## ğŸ¨ **AnvÃ¤ndarupplevelse**

### **Frontend (Next.js + shadcn/ui)**
**Huvudfunktioner:**
- **SÃ¶kfÃ¤lt:** AnvÃ¤ndare skriver in spel-titlar
- **Rekommendationer:** Visar liknande spel med:
  - Spel-titel och beskrivning
  - Cover-bild
  - Genres och teman
  - Rating och releasedatum
  - Likhetsscore
- **Filtrering:** Filtrera pÃ¥ genre, plattform, Ã¥r
- **Responsiv design:** Fungerar pÃ¥ desktop och mobil

### **API Endpoints (FastAPI)**
```python
# Huvudendpoints:
GET /api/games/search?query={game_name}
GET /api/games/recommendations?game_id={id}&limit={n}
GET /api/games/{id}
GET /api/genres
GET /api/platforms
POST /api/recommendations/batch  # FÃ¶r flera spel samtidigt
```

### **Frontend Development & Empty State Handling** â­ **IMPLEMENTERAT** âœ…

**Problem som lÃ¶stes:**
- Mock-data var fÃ¶rvirrande och visade inte verklig data
- API kunde inte ladda NDJSON-format korrekt
- Ingen tydlig feedback nÃ¤r data saknades

**LÃ¶sningar implementerade:**
- **API Data Loading:** StÃ¶d fÃ¶r NDJSON-format och automatisk data type conversion
- **Empty State Management:** Proper loading, error och empty states med retry funktionalitet
- **Mock Data Removal:** Borttaget all mock-data och ersatt med riktig IGDB API data
- **User Experience:** Tydliga meddelanden och retry-knappar fÃ¶r bÃ¤ttre anvÃ¤ndarupplevelse

**Resultat:**
- âœ… Riktig data frÃ¥n IGDB API (100 games)
- âœ… Tydliga meddelanden nÃ¤r data saknas eller fel uppstÃ¥r
- âœ… Clean code utan fÃ¶rvirrande mock-data logik

---

## ğŸš€ **Utvecklingsfaser**

### **Fas 1: Frontend-First Prototyping** â­ **KLAR** âœ…
**MÃ¥l:** Visuell feedback och iterativ utveckling

**Uppgifter:**
- [x] Skapa projektstruktur enligt best practice
- [x] Migrera Isaks IGDB API kod till `src/api/`
- [x] Utveckla data collection script
- [x] Bygg data preprocessing pipeline
- [x] **Frontend setup** med Next.js + shadcn/ui
- [x] **Data visualization** - visa testdata i tables/charts
- [x] **Budget tracking** dashboard fÃ¶r GCP credits
- [x] **Basic API endpoints** fÃ¶r data access
- [x] **GCP Integration** - budget monitoring med verklig data
- [x] **Empty State Handling** - proper error handling och user feedback
- [x] **API Data Loading** - NDJSON support och data type conversion

### **Fas 2: Local-First ML Development** â­ **KLAR** âœ…
**MÃ¥l:** Bygga robust rekommendationsmotor lokalt innan cloud scaling

**Strategi:** "Progressive Local-First" - utveckla och testa allt lokalt fÃ¶rst

**Uppgifter:**
- [x] **Data Collection (1,000+ spel)** - samla tillrÃ¤ckligt med data lokalt
- [x] **Progressive feature engineering** - bÃ¶rja med core features (genres, themes)
- [x] **Local model training** pÃ¥ MacBook med scikit-learn
- [x] **Manual evaluation system** - "Ser dessa rekommendationer rimliga ut?"
- [x] **Frontend integration** - sÃ¶k + rekommendationer i UI
- [x] **Model comparison** - testa olika algoritmer visuellt
- [x] **Performance optimization** fÃ¶r lokala constraints
- [x] **Data quality validation** med visuell feedback

### **Fas 3: Docker & CI/CD Integration** â­ **KLAR** âœ…
**MÃ¥l:** Containerisering och CI/CD-pipeline fÃ¶r skalning till molnet

**Uppgifter:**
- [x] **GCP budget tracking** - real-time cost monitoring
- [x] **Docker containerization** - Frontend + Backend + PostgreSQL
- [x] **TypeScript/ESLint fixes** - Clean builds utan fel
- [x] **Lokal Docker-testning** - Alla services fungerar perfekt
- [x] **GitHub Actions CI/CD** - Simple CI pipeline implementerad och fungerar
- [x] **GitHub CLI Integration** - Direkt workflow-Ã¶vervakning frÃ¥n terminal
- [x] **Python Code Quality** - Black, flake8, isort automation
- [x] **Pre-commit Hooks** - Lokal kodkvalitet fÃ¶re commit
- [x] **Status Badges** - Real-time CI/CD status i README
- [x] **Frontend Component Fixes** - TypeScript path mapping fixade, Docker build fungerar

### **Fas 4: Airflow Data Pipeline** â­ **KLAR** âœ…
**MÃ¥l:** Automatiserad data pipeline frÃ¥n IGDB API till ML-modeller

**Uppgifter:**
- [x] **Airflow 3.0 Installation** - Apache Airflow med Google providers
- [x] **Cloud Storage Setup** - GCS buckets fÃ¶r raw och processed data
- [x] **Airflow DAG Development** - Komplett pipeline DAG implementerad
- [x] **Data Collection Task** - IGDB API â†’ Local JSON files
- [x] **Storage Upload Task** - Local files â†’ Google Cloud Storage
- [x] **BigQuery Load Task** - GCS â†’ BigQuery raw tables
- [x] **dbt Integration Task** - Raw data â†’ ML-ready features
- [x] **ML Training Task** - Transformed data â†’ Trained models
- [x] **Test DAG** - Verifiering av alla komponenter
- [x] **GCP Authentication** - Service account integration
- [x] **Airflow Configuration** - JWT secrets och sÃ¤kra nycklar
- [x] **Web UI Access** - http://localhost:8080 fungerar perfekt
- [x] **Documentation** - Komplett setup guide i docs/AIRFLOW_SETUP.md

### **Fas 4: Airflow Data Pipeline** â­ **KLAR** âœ…
**MÃ¥l:** Automatiserad data pipeline frÃ¥n IGDB API till ML-modeller

**Implementation:**
- **Airflow 3.0** - Apache Airflow med Google providers
- **Cloud Storage** - GCS buckets fÃ¶r raw och processed data
- **DAG Development** - Komplett pipeline med 5 tasks
- **GCP Integration** - Service account authentication
- **Web UI** - http://localhost:8080 fÃ¶r monitoring
- **Documentation** - Komplett setup guide i docs/AIRFLOW_SETUP.md

**Pipeline Tasks:**
1. **collect_igdb_data** - IGDB API â†’ Local JSON files
2. **upload_to_gcs** - Local files â†’ Google Cloud Storage
3. **load_to_bigquery** - GCS â†’ BigQuery raw tables
4. **run_dbt_transformations** - Raw data â†’ ML-ready features
5. **train_ml_models** - Transformed data â†’ Trained models

### **Fas 5: Vertex AI Learning & Progressive Scaling** â­ **NÃ„STA** ğŸ¯
**Strategi:** "Local Testing â†’ Vertex AI Learning â†’ Progressive Scaling â†’ Cloud Production"

**MÃ¥l:** LÃ¤r dig GCP ML-tjÃ¤nster med vÃ¥ra 100 spel, skala successivt med kvalitetskontroll

#### **Fas 4A: GCP Learning (1-2 dagar)** âœ… **KLAR**
**Syfte:** LÃ¤r dig BigQuery, dbt, Airflow, Vertex AI med vÃ¥ra 100 spel

**Steg 1: BigQuery Setup** âœ… **KLAR**
- [x] **Skapa BigQuery dataset** `igdb_games` (exalted-tempo-471613-e2)
- [x] **Ladda upp vÃ¥ra 100 spel** frÃ¥n lokal CSV till BigQuery (games_raw tabell)
- [x] **Testa SQL queries** pÃ¥ speldata i BigQuery (83 kolumner, 100 spel)
- [x] **Skapa views** fÃ¶r genres, platforms, themes (genre_analysis, platform_analysis)

**Steg 2: dbt Project Setup** âœ… **KLAR**
- [x] **Skapa dbt project** fÃ¶r data transformation (`dbt_igdb_project/igdb_models`)
- [x] **Definiera models** fÃ¶r games, genres, platforms (stg_games, game_recommendations)
- [x] **Testa transformations** lokalt med dbt (100 spel transformerade)
- [x] **Deploy till BigQuery** med dbt (US region, igdb_games dataset)

**Steg 3: Airflow DAG Setup** âœ… **KLAR**
- [x] **Skapa Airflow DAG** fÃ¶r data pipeline (igdb_data_pipeline.py)
- [x] **Definiera tasks** fÃ¶r data collection, transformation, ML (5 tasks)
- [x] **Testa DAG** lokalt med Airflow (Web UI pÃ¥ localhost:8080)
- [x] **Deploy till Cloud Composer** (eller lokal Airflow)

**Steg 4: EU Migration & AutoML Pipeline** âœ… **KLAR** / ğŸ”„ **NÃ„STA**
- [x] **EU Migration Complete** - BigQuery dataset `igdb_game_data` i EU region
- [x] **EU Storage buckets** - Raw och processed data i europe-west1
- [x] **dbt EU konfiguration** - OAuth authentication fungerar
- [ ] **AutoML integration** fÃ¶r automatisk modelltrÃ¤ning
- [ ] **Incremental data collection** fÃ¶r resursoptimering
- [ ] **CI/CD GCP deployment** fÃ¶r production pipeline

---

## ğŸ› ï¸ **Detaljerad Implementation Guide - Fas 5: Vertex AI Learning**

### **Steg 1: Airflow DAG Testing (1-2 timmar)** ğŸ”„ **NÃ„STA**

#### **1.1 Starta Airflow lokalt**
```bash
# Aktivera venv fÃ¶rst
source venv/bin/activate

# Starta Airflow
./airflow/start_airflow.sh

# Verifiera att Airflow kÃ¶rs
curl http://localhost:8080/health
```

#### **1.2 Testa Airflow DAG**
**Ã–ppna Airflow Web UI:**
- GÃ¥ till http://localhost:8080
- Login: admin / [genererat lÃ¶senord frÃ¥n start_airflow.sh]
- Hitta DAG: `igdb_data_pipeline`

**Testa DAG manuellt:**
1. **Klicka pÃ¥ DAG namnet** â†’ `igdb_data_pipeline`
2. **Klicka pÃ¥ "Trigger DAG"** (play-knapp)
3. **VÃ¤lj "Trigger DAG w/ Config"** fÃ¶r att skicka parametrar
4. **Konfigurera:**
   ```json
   {
     "games_limit": 100,
     "test_mode": true
   }
   ```

**Ã–vervaka kÃ¶rning:**
- **Graph View:** Se task dependencies och status
- **Tree View:** Se historik Ã¶ver kÃ¶rningar
- **Logs:** Klicka pÃ¥ task â†’ "Log" fÃ¶r detaljerade felmeddelanden

#### **1.3 Troubleshooting vanliga problem**
```bash
# Om DAG inte visas:
# Kontrollera att DAG-filen Ã¤r korrekt placerad
ls -la airflow/dags/igdb_data_pipeline.py

# Om tasks failar:
# Klicka pÃ¥ task â†’ "Log" fÃ¶r att se felmeddelanden
# Vanliga problem:
# - IGDB API credentials saknas
# - GCP service account key saknas
# - Python path problem
```

### **Steg 2: Vertex AI Notebook Setup (1-2 timmar)**

#### **2.1 Aktivera Vertex AI API**
```bash
# Aktivera venv fÃ¶rst
source venv/bin/activate

# Aktivera Vertex AI API
gcloud services enable aiplatform.googleapis.com

# Verifiera att API Ã¤r aktiverat
gcloud services list --enabled --filter="name:aiplatform"
```

#### **2.2 Skapa Vertex AI Notebook Instance**
```bash
# Skapa notebook instance
gcloud ai notebooks instances create igdb-ml-notebook \
  --location=europe-west1 \
  --machine-type=e2-standard-4 \
  --vm-image-project=deeplearning-platform-release \
  --vm-image-family=tf2-2-8-cpu \
  --vm-image-name=tf2-2-8-cpu-20220119-170516

# Ã–ppna notebook
gcloud ai notebooks instances open igdb-ml-notebook --location=europe-west1
```

#### **2.3 Konfigurera Notebook Environment**
```python
# Installera dependencies i notebook
!pip install pandas numpy scikit-learn google-cloud-bigquery

# Konfigurera GCP authentication
from google.cloud import bigquery
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Testa BigQuery connection
client = bigquery.Client()
print("BigQuery connection successful!")
```

### **Steg 3: ML Model Comparison (1-2 timmar)**

#### **3.1 Ladda data frÃ¥n BigQuery**
```python
# Ladda vÃ¥ra 100 spel frÃ¥n BigQuery
query = """
SELECT id, name, summary, rating, genre_id, platform_id, theme_id
FROM `exalted-tempo-471613-e2.igdb_games.game_recommendations`
WHERE summary IS NOT NULL
"""
df = client.query(query).to_dataframe()
print(f"Laddat {len(df)} spel frÃ¥n BigQuery")
```

#### **3.2 TrÃ¤na content-based model**
```python
# TrÃ¤na samma modell som lokalt
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
tfidf_matrix = vectorizer.fit_transform(df['summary'].fillna(''))

# BerÃ¤kna similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

print(f"TrÃ¤nat model med {len(df)} spel")
print(f"Similarity matrix shape: {similarity_matrix.shape}")
```

#### **3.3 JÃ¤mfÃ¶r prestanda**
```python
# Testa rekommendationer
def get_recommendations(game_id, top_n=3):
    game_idx = df[df['id'] == game_id].index[0]
    similarity_scores = similarity_matrix[game_idx]
    top_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]
    return df.iloc[top_indices][['id', 'name', 'rating']]

# Testa med ett spel
test_game_id = df.iloc[0]['id']
recommendations = get_recommendations(test_game_id)
print("Rekommendationer:")
print(recommendations)
```

### **Steg 4: Progressive Scaling Strategy (2-3 dagar)**

#### **4.1 Skalningsfaser**
**Fas 1: 100 â†’ 1,000 spel (2-3 timmar)**
- Testa lokalt fÃ¶rst
- Verifiera data quality
- JÃ¤mfÃ¶r ML prestanda

**Fas 2: 1,000 â†’ 10,000 spel (4-6 timmar)**
- Implementera batch processing
- Flytta data till Cloud Storage
- Testa Vertex AI trÃ¤ning

**Fas 3: 10,000 â†’ 100,000+ spel (1-2 dagar)**
- Cloud Functions fÃ¶r parallell data collection
- Vertex AI fÃ¶r distributed ML training
- BigQuery fÃ¶r data processing

#### **4.2 Kvalitetskontroll vid varje steg**
```bash
# Data quality validation
python -c "from src.data_processing.etl_pipeline import ETLPipeline; ETLPipeline().validate_data()"

# ML performance testing
python -c "from src.models.game_recommender import GameRecommender; GameRecommender().train_and_evaluate()"

# Airflow DAG testing
# KÃ¶r DAG med nya parametrar och Ã¶vervaka resultat
```

---

## ğŸ› ï¸ **Detaljerad Implementation Guide - Fas 4A**

### **Steg 1: BigQuery Setup (2-3 timmar)**

#### **1.1 Aktivera BigQuery API**
```bash
# Aktivera venv fÃ¶rst
source venv/bin/activate

# Aktivera BigQuery API
gcloud services enable bigquery.googleapis.com

# Verifiera att API Ã¤r aktiverat
gcloud services list --enabled --filter="name:bigquery"
```

#### **1.2 Skapa BigQuery Dataset**
```bash
# Skapa dataset fÃ¶r vÃ¥r data
bq mk --dataset --location=EU --description="IGDB Game Data for ML Pipeline" \
  exalted-tempo-471613-e2:igdb_game_data

# Verifiera att dataset skapades
bq ls exalted-tempo-471613-e2:igdb_game_data
```

#### **1.3 Ladda upp vÃ¥ra 100 spel**
```bash
# Konvertera vÃ¥r CSV till BigQuery format
python -c "
import pandas as pd
import json

# LÃ¤s vÃ¥r senaste processed data
df = pd.read_csv('data/processed/games_20250910_194944.csv')
print(f'Laddar {len(df)} spel till BigQuery...')

# Spara som JSON fÃ¶r BigQuery
df.to_json('games_for_bigquery.json', orient='records', lines=True)
print('Data konverterad till JSON format')
"

# Ladda upp till BigQuery
bq load --source_format=NEWLINE_DELIMITED_JSON \
  --autodetect \
  exalted-tempo-471613-e2:igdb_game_data.games \
  games_for_bigquery.json

# Verifiera att data laddades
bq query --use_legacy_sql=false "
SELECT COUNT(*) as total_games, 
       COUNT(DISTINCT genre_id) as unique_genres,
       COUNT(DISTINCT platform_id) as unique_platforms
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
"
```

#### **1.4 Skapa Views fÃ¶r Analysis**
```bash
# Skapa view fÃ¶r genre analysis
bq query --use_legacy_sql=false "
CREATE OR REPLACE VIEW \`exalted-tempo-471613-e2.igdb_game_data.genre_analysis\` AS
SELECT 
  genre_id,
  COUNT(*) as game_count,
  AVG(rating) as avg_rating,
  MIN(release_year) as earliest_year,
  MAX(release_year) as latest_year
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
WHERE genre_id IS NOT NULL
GROUP BY genre_id
ORDER BY game_count DESC
"

# Skapa view fÃ¶r platform analysis
bq query --use_legacy_sql=false "
CREATE OR REPLACE VIEW \`exalted-tempo-471613-e2.igdb_game_data.platform_analysis\` AS
SELECT 
  platform_id,
  COUNT(*) as game_count,
  AVG(rating) as avg_rating,
  MIN(release_year) as earliest_year,
  MAX(release_year) as latest_year
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
WHERE platform_id IS NOT NULL
GROUP BY platform_id
ORDER BY game_count DESC
"
```

### **Steg 2: dbt Project Setup (2-3 timmar)** âœ… **KLAR**

#### **2.1 Installera dbt** âœ… **KLAR**
```bash
# Installera dbt med BigQuery support
pip install dbt-bigquery

# Verifiera installation
dbt --version
```

#### **2.2 Skapa dbt Project** âœ… **KLAR**
```bash
# Skapa dbt project
mkdir dbt_igdb_project
cd dbt_igdb_project

# Initiera dbt project
dbt init igdb_models

# Konfigurera profiles.yml fÃ¶r BigQuery
# Dataset: igdb_games (US region)
# Project: exalted-tempo-471613-e2
# Authentication: Service Account
```

#### **2.3 Skapa dbt Models** âœ… **KLAR**

**Staging Model (`stg_games.sql`):**
- Rensar och standardiserar raw games data
- Data quality checks och derived fields
- Rating categories (Excellent, Good, Average, etc.)
- Era categories (Recent, Modern, Classic, Retro)

**Marts Model (`game_recommendations.sql`):**
- ML-optimerad tabell fÃ¶r rekommendationer
- Feature vectors fÃ¶r genre similarity
- Imputed values fÃ¶r missing data
- Processed timestamps fÃ¶r data freshness

#### **2.4 dbt Configuration** âœ… **KLAR**
```yaml
# dbt_project.yml
models:
  igdb_models:
    staging:
      +materialized: view
    marts:
      +materialized: table
```

#### **2.5 Data Tests** âœ… **KLAR**
- Unique constraints pÃ¥ game IDs
- Not-null validations pÃ¥ kritiska fÃ¤lt
- Source data quality tests
- 12/13 tests passerade (1 example test misslyckades)

#### **2.6 Dataset Configuration** âœ… **KLAR**
**BigQuery Datasets:**
- `igdb_games` (US region) - âœ… **AKTIV** - VÃ¥r data och dbt models
- `igdb_game_data` (EU region) - âš ï¸ **OANVÃ„ND** - Skapad tidigare, kan tas bort

**Region Val:**
- **US region** vald fÃ¶r bÃ¤ttre prestanda med GCP-tjÃ¤nster
- **EU region** hade krÃ¤vt extra konfiguration fÃ¶r andra GCP-tjÃ¤nster
- **Rekommendation:** BehÃ¥ll US region fÃ¶r konsistens

#### **2.3 Konfigurera dbt Profile**
```bash
# Skapa profiles.yml
mkdir -p ~/.dbt
cat > ~/.dbt/profiles.yml << 'EOF'
igdb_bigquery:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      keyfile: /path/to/your/service-account-key.json
      project: exalted-tempo-471613-e2
      dataset: igdb_game_data
      location: EU
      threads: 4
      timeout_seconds: 300
EOF
```

#### **2.4 Skapa dbt Models**
```bash
# Skapa staging models
mkdir -p models/staging
cat > models/staging/stg_games.sql << 'EOF'
-- Staging model fÃ¶r games data
SELECT 
  id,
  name,
  summary,
  rating,
  release_year,
  genre_id,
  platform_id,
  theme_id,
  created_at,
  updated_at
FROM {{ source('raw', 'games') }}
WHERE name IS NOT NULL
EOF

# Skapa marts models
mkdir -p models/marts
cat > models/marts/game_recommendations.sql << 'EOF'
-- Mart model fÃ¶r game recommendations
SELECT 
  g.id,
  g.name,
  g.summary,
  g.rating,
  g.release_year,
  g.genre_id,
  g.platform_id,
  g.theme_id,
  CASE 
    WHEN g.rating >= 80 THEN 'Excellent'
    WHEN g.rating >= 70 THEN 'Good'
    WHEN g.rating >= 60 THEN 'Average'
    ELSE 'Below Average'
  END as rating_category
FROM {{ ref('stg_games') }} g
WHERE g.rating IS NOT NULL
EOF
```

#### **2.5 Testa dbt**
```bash
# Testa dbt connection
dbt debug

# KÃ¶r dbt models
dbt run

# Testa dbt models
dbt test

# Generera dokumentation
dbt docs generate
dbt docs serve
```

### **Steg 3: Airflow DAG Setup (2-3 timmar)**

#### **3.1 Installera Airflow**
```bash
# Installera Airflow
pip install apache-airflow

# Initiera Airflow
airflow db init

# Skapa admin user
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin
```

#### **3.2 Skapa Airflow DAG**
```bash
# Skapa DAGs mapp
mkdir -p ~/airflow/dags

# Skapa IGDB data pipeline DAG
cat > ~/airflow/dags/igdb_data_pipeline.py << 'EOF'
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'igdb-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 11),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'igdb_data_pipeline',
    default_args=default_args,
    description='IGDB Game Data Collection and Processing Pipeline',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

def collect_game_data():
    """Collect game data from IGDB API"""
    import subprocess
    subprocess.run(['python', 'collect_data.py', '--games-limit', '100'], cwd='/path/to/your/project')
    return "Data collection completed"

def process_data():
    """Process collected data"""
    import subprocess
    subprocess.run(['python', '-m', 'src.data_processing.etl_pipeline'], cwd='/path/to/your/project')
    return "Data processing completed"

def train_model():
    """Train ML model"""
    import subprocess
    subprocess.run(['python', '-m', 'src.models.train_recommender'], cwd='/path/to/your/project')
    return "Model training completed"

# Define tasks
collect_task = PythonOperator(
    task_id='collect_game_data',
    python_callable=collect_game_data,
    dag=dag,
)

process_task = PythonOperator(
    task_id='process_data',
    python_callable=process_data,
    dag=dag,
)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag,
)

# Define task dependencies
collect_task >> process_task >> train_task
EOF
```

#### **3.3 Starta Airflow**
```bash
# Starta Airflow webserver
airflow webserver --port 8080 &

# Starta Airflow scheduler
airflow scheduler &

# Ã–ppna Airflow UI
open http://localhost:8080
```

### **Steg 4: Vertex AI Learning (2-3 timmar)**

#### **4.1 Aktivera Vertex AI API**
```bash
# Aktivera Vertex AI API
gcloud services enable aiplatform.googleapis.com

# Verifiera att API Ã¤r aktiverat
gcloud services list --enabled --filter="name:aiplatform"
```

#### **4.2 Skapa Vertex AI Notebook**
```bash
# Skapa Vertex AI notebook instance
gcloud ai notebooks instances create igdb-ml-notebook \
  --location=europe-west1 \
  --machine-type=e2-standard-4 \
  --vm-image-project=deeplearning-platform-release \
  --vm-image-family=tf2-2-8-cpu \
  --vm-image-name=tf2-2-8-cpu-20220119-170516

# Ã–ppna notebook
gcloud ai notebooks instances open igdb-ml-notebook --location=europe-west1
```

#### **4.3 Testa ML trÃ¤ning i Vertex AI**
```python
# Skapa notebook cell fÃ¶r ML trÃ¤ning
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from google.cloud import bigquery

# Ladda data frÃ¥n BigQuery
client = bigquery.Client()
query = """
SELECT id, name, summary, rating, genre_id, platform_id, theme_id
FROM `exalted-tempo-471613-e2.igdb_game_data.games`
WHERE summary IS NOT NULL
"""
df = client.query(query).to_dataframe()

# TrÃ¤na enkel content-based model
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(df['summary'].fillna(''))

# BerÃ¤kna similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

print(f"TrÃ¤nat model med {len(df)} spel")
print(f"Similarity matrix shape: {similarity_matrix.shape}")
```

---

## ğŸš€ **Skalningsstrategi: 100 â†’ 100,000+ Spel**

### **Kritiska Skalningsutmaningar & LÃ¶sningar**

#### **1. Data Collection Bottlenecks** ğŸ”¥ **KRITISKT**
**Problem:** IGDB API rate limit (30 req/min) blir flaskhals vid 100,000+ spel
**LÃ¶sningar:**
- **Batch Processing:** Samla data i chunks (1000 spel per batch)
- **Parallel Processing:** AnvÃ¤nda Cloud Functions fÃ¶r parallell data collection
- **Caching Strategy:** Redis/Memcached fÃ¶r att undvika duplicerade API calls
- **Incremental Updates:** Bara samla nya/uppdaterade spel dagligen

#### **2. Storage & Processing Limits** ğŸ’¾ **VIKTIGT**
**Problem:** Lokal storage och minne rÃ¤cker inte fÃ¶r 100,000+ spel
**LÃ¶sningar:**
- **Cloud Storage:** GCS buckets fÃ¶r raw data (billigare Ã¤n BigQuery)
- **Data Partitioning:** Partitionera data per Ã¥r/genre fÃ¶r snabbare queries
- **Streaming Processing:** Cloud Dataflow fÃ¶r real-time data processing
- **Data Archiving:** Flytta gamla data till Coldline Storage

#### **3. ML Model Performance** ğŸ¤– **KRITISKT**
**Problem:** Cosine similarity pÃ¥ 100,000+ spel blir extremt lÃ¥ngsam
**LÃ¶sningar:**
- **Vector Databases:** Pinecone/Weaviate fÃ¶r snabba similarity searches
- **Model Optimization:** AnvÃ¤nda approximate nearest neighbors (ANN)
- **Feature Selection:** Reducera dimensions med PCA/t-SNE
- **Distributed Training:** Vertex AI fÃ¶r parallell modelltrÃ¤ning

#### **4. API Response Times** âš¡ **VIKTIGT**
**Problem:** Rekommendationer tar fÃ¶r lÃ¥ng tid med stora datasets
**LÃ¶sningar:**
- **Precomputed Recommendations:** BerÃ¤kna rekommendationer i fÃ¶rvÃ¤g
- **Caching Layer:** Redis fÃ¶r snabba API responses
- **CDN:** CloudFlare fÃ¶r statisk content
- **Database Indexing:** Optimera BigQuery queries med proper indexing

### **Best Practice Architecture fÃ¶r Skalning**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IGDB API      â”‚â”€â”€â”€â–¶â”‚  Cloud Functions â”‚â”€â”€â”€â–¶â”‚  Cloud Storage  â”‚
â”‚  (Rate Limited) â”‚    â”‚  (Parallel Jobs) â”‚    â”‚   (Raw Data)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vertex AI     â”‚â—€â”€â”€â”€â”‚   Airflow DAG    â”‚â—€â”€â”€â”€â”‚   BigQuery      â”‚
â”‚  (ML Training)  â”‚    â”‚ (Orchestration)  â”‚    â”‚ (Processed Data)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloud Run     â”‚â—€â”€â”€â”€â”‚   Redis Cache    â”‚â—€â”€â”€â”€â”‚   Vector DB     â”‚
â”‚  (API Serving)  â”‚    â”‚ (Fast Responses) â”‚    â”‚ (Similarity)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Skalningsfaser**

#### **Fas 1: 100 â†’ 1,000 spel** (Nuvarande)
- âœ… BigQuery setup klar
- ğŸ”„ dbt project fÃ¶r data transformation
- ğŸ”„ Airflow DAG fÃ¶r automation
- ğŸ”„ Basic ML model optimization

#### **Fas 2: 1,000 â†’ 10,000 spel** (NÃ¤sta vecka)
- ğŸ”„ Cloud Storage integration
- ğŸ”„ Parallel data collection
- ğŸ”„ Model performance optimization
- ğŸ”„ Caching implementation

#### **Fas 3: 10,000 â†’ 100,000+ spel** (Framtida)
- ğŸ”„ Vector database integration
- ğŸ”„ Distributed ML training
- ğŸ”„ Advanced caching strategies
- ğŸ”„ Production monitoring

---

## ğŸ¯ **NÃ¤sta Steg efter Fas 4A**

NÃ¤r du har genomfÃ¶rt Fas 4A kommer du att ha:
- âœ… **BigQuery dataset** med vÃ¥ra 100 spel
- âœ… **dbt project** fÃ¶r data transformation
- âœ… **Airflow DAG** fÃ¶r data pipeline
- âœ… **Vertex AI notebook** fÃ¶r ML experimentation

**DÃ¥ Ã¤r du redo fÃ¶r Fas 4B: Local Scaling** med 10,000+ spel!

#### **Fas 4B: Local Scaling (1-2 dagar)** ğŸ”„
**Syfte:** Skala upp till 10,000+ spel lokalt nÃ¤r du fÃ¶rstÃ¥r GCP

**Steg 1: Data Collection Scaling**
- [ ] **Samla 10,000+ spel** med `collect_data.py --games-limit 10000`
- [ ] **Optimera data collection** fÃ¶r stÃ¶rre volymer
- [ ] **Implementera batch processing** fÃ¶r effektivitet
- [ ] **Validera data quality** med stÃ¶rre dataset

**Steg 2: ML Algorithm Optimization**
- [ ] **Testa olika ML-algoritmer** med 10,000+ spel
- [ ] **Optimera feature engineering** fÃ¶r prestanda
- [ ] **Implementera model evaluation** med cross-validation
- [ ] **JÃ¤mfÃ¶r algoritmer** (content-based vs collaborative)

**Steg 3: Performance Optimization**
- [ ] **Optimera Docker builds** fÃ¶r snabbare deployment
- [ ] **Implementera caching** fÃ¶r API responses
- [ ] **Optimera database queries** fÃ¶r prestanda
- [ ] **Testa load testing** med stÃ¶rre datamÃ¤ngder

#### **Fas 4C: Cloud Production (1-2 dagar)** ğŸš€
**Syfte:** Deploy komplett system till GCP nÃ¤r allt fungerar lokalt

**Steg 1: Cloud Run Deployment**
- [ ] **Deploy frontend** till Cloud Run
- [ ] **Deploy backend** till Cloud Run
- [ ] **Konfigurera Cloud SQL** fÃ¶r PostgreSQL
- [ ] **Testa end-to-end** i molnet

**Steg 2: Data Pipeline Production**
- [ ] **Deploy Airflow DAG** till Cloud Composer
- [ ] **Konfigurera BigQuery** fÃ¶r production data
- [ ] **Deploy dbt models** fÃ¶r data transformation
- [ ] **Automatisera data pipeline** med Airflow

**Steg 3: ML Production**
- [ ] **Deploy ML model** till Vertex AI
- [ ] **Konfigurera model serving** med Cloud Run
- [ ] **Implementera model monitoring** med Vertex AI
- [ ] **Automatisera model retraining** med Airflow

**Steg 4: Production Monitoring**
- [ ] **SÃ¤tt upp Cloud Monitoring** fÃ¶r system health
- [ ] **Konfigurera budget alerts** fÃ¶r kostnadskontroll
- [ ] **Implementera logging** med Cloud Logging
- [ ] **SÃ¤tt upp error tracking** med Cloud Error Reporting

### **Fas 5: Advanced ML & Production** â­ **FRAMTIDA** ğŸ”®
**MÃ¥l:** Production-ready system med avancerade funktioner

**Uppgifter:**
- [ ] **Advanced features** - text analysis, visual similarity
- [ ] **A/B testing** framework med frontend integration
- [ ] **User feedback** system fÃ¶r continuous improvement
- [ ] **Real-time rekommendationer** med caching
- [ ] **Performance monitoring** med budget tracking
- [ ] **CI/CD pipeline** med automated testing
- [ ] **Documentation** och presentation fÃ¶r kursen

---

## ğŸ“ˆ **Success Metrics**

### **Tekniska Metrics**
- **API Response Time:** < 200ms fÃ¶r rekommendationer
- **Data Freshness:** Daglig uppdatering av speldata
- **System Uptime:** > 99.5%
- **Model Accuracy:** > 80% relevanta rekommendationer

### **AnvÃ¤ndarupplevelse Metrics**
- **Search Success Rate:** > 90% hittar sÃ¶kta spel
- **Recommendation Relevance:** AnvÃ¤ndarfeedback > 4/5
- **Page Load Time:** < 2 sekunder
- **Mobile Responsiveness:** Fungerar pÃ¥ alla enheter

### **Business Metrics**
- **Data Pipeline Efficiency:** < 1 timme fÃ¶r fullstÃ¤ndig datauppdatering
- **Cost Optimization:** < $100/mÃ¥nad i GCP-kostnader (med budget tracking)
- **Scalability:** StÃ¶der 1000+ samtidiga anvÃ¤ndare
- **Budget Utilization:** < 80% av tillgÃ¤ngliga GCP credits
- **Development Velocity:** Visuell feedback inom 1 dag fÃ¶r varje feature

---

## ğŸ¯ **NÃ¤sta Steg - Hybrid GCP Strategy**

### **Omedelbara Ã¥tgÃ¤rder (Idag):**
1. **Fas 4A: GCP Learning** - BÃ¶rja med BigQuery setup
2. **Ladda upp vÃ¥ra 100 spel** till BigQuery fÃ¶r att lÃ¤ra sig
3. **Skapa dbt project** fÃ¶r data transformation
4. **Testa Airflow DAG** lokalt

### **Denna vecka:**
- **Fas 4A:** GCP Learning (BigQuery, dbt, Airflow, Vertex AI)
- **Fas 4B:** Local Scaling (10,000+ spel lokalt)
- **Fas 4C:** Cloud Production (deploy till GCP)

### **Kommande veckor:**
- **Vecka 4:** Advanced ML och production deployment
- **Vecka 5:** Kurs presentation och dokumentation

---

## ğŸ“ **Projektstatus**

**Senast uppdaterad:** 2025-09-12
**Nuvarande fas:** Fas 5 - AutoML Pipeline & Cloud Production (ğŸ”„ PÃ¥gÃ¥ende) + EU Migration Complete (âœ… Klar)
**NÃ¤sta milestone:** Skalbar AutoML pipeline med incremental data collection och CI/CD deployment
**Gruppmedlemmar:** Viktoria, Isak & Johan
**Teknisk stack:** Python, Next.js, shadcn/ui, Docker, GCP, IGDB API
**Budget:** AI24S-Data-Engineering-IGDB (kr100.00/mÃ¥nad) + $300 GCP credits
**GCP Project:** IGDB-ML-Pipeline (exalted-tempo-471613-e2)
**Strategi:** Hybrid GCP Learning â†’ Local Scaling â†’ Cloud Production
**Status:** Komplett fungerande system med 100 spel, ML-rekommendationer, data quality dashboard, Docker containerization och EU GCP migration. Redo fÃ¶r AutoML pipeline och production deployment.

---

## ğŸš€ **NÃ¤sta Steg - AutoML Pipeline & Production Deployment**

### **Fas 5B: Skalbar AutoML Pipeline** ğŸ¯ **NÃ„STA**

**MÃ¥l:** Implementera automatisk, skalbar ML pipeline som fungerar lika bra med 100 spel som med 10,000+ spel

#### **Steg 1: AutoML Integration (1-2 dagar)**
- [ ] **Konfigurera Vertex AI AutoML** fÃ¶r automatisk modelltrÃ¤ning
- [ ] **Integrera AutoML med Airflow DAG** fÃ¶r automatiserad pipeline
- [ ] **Testa AutoML prestanda** med vÃ¥ra 100 spel frÃ¥n BigQuery EU
- [ ] **JÃ¤mfÃ¶r AutoML vs manuell ML** fÃ¶r prestanda och kostnad

#### **Steg 2: Incremental Data Collection (1 dag)**
- [ ] **Implementera data freshness tracking** i BigQuery
- [ ] **Optimera IGDB API calls** fÃ¶r att undvika duplicerade requests
- [ ] **Caching strategy** fÃ¶r att spara API rate limits
- [ ] **Batch processing** fÃ¶r effektiv data collection

#### **Steg 3: CI/CD GCP Deployment (1-2 dagar)**
- [ ] **Konfigurera Cloud Build** fÃ¶r automatisk deployment
- [ ] **Deploy frontend till Cloud Run** med Next.js
- [ ] **Deploy backend till Cloud Run** med FastAPI
- [ ] **Konfigurera Cloud SQL** fÃ¶r PostgreSQL production
- [ ] **SÃ¤tt upp monitoring** med Cloud Monitoring

#### **Steg 4: Production Monitoring (1 dag)**
- [ ] **Budget alerts** fÃ¶r kostnadskontroll
- [ ] **Performance monitoring** fÃ¶r API response times
- [ ] **Error tracking** med Cloud Error Reporting
- [ ] **Logging** med Cloud Logging

### **Teknisk Arkitektur fÃ¶r Production:**
```
IGDB API â†’ Airflow â†’ BigQuery EU â†’ AutoML â†’ Trained Model â†’ Cloud Run â†’ Next.js Frontend
```

**FÃ¶rdelar med AutoML:**
- âœ… Automatisk skalning med datamÃ¤ngd
- âœ… Ingen manuell ML-optimering krÃ¤vs
- âœ… GCP hanterar infrastruktur
- âœ… Konsistent prestanda oavsett data-volym

---

## ğŸ¯ **UPPDATERAD GCP DEPLOYMENT STRATEGI**

### **Kursprojekt-fokus (4 veckor kvar):**
- ğŸ“ **BehÃ¶ver fungerande pipeline** fÃ¶r betyg
- ğŸ’° **$300 free credits** - Vill inte brÃ¤nna i onÃ¶dan
- ğŸ¯ **100 spel** - Nuvarande dataset, vill expandera senare
- ğŸ“š **LÃ¤rande-fokus** - Vill fÃ¶rstÃ¥ varje steg

### **Kostnadseffektiv approach:**
**Alternativ 1: Enkel Pipeline (Rekommenderat)**
```
IGDB API â†’ Cloud Functions â†’ BigQuery â†’ Cloud Run (FastAPI + Next.js)
```
- **Kostnad:** ~$35/mÃ¥nad
- **Setup-tid:** 1-2 dagar
- **Skalbar:** Fungerar fÃ¶r 100 spel och 334,000 spel

**Alternativ 2: Cloud Composer Pipeline (Senare)**
```
IGDB API â†’ Cloud Functions â†’ BigQuery â†’ dbt â†’ Vertex AI â†’ Cloud Run
```
- **Kostnad:** ~$330/mÃ¥nad
- **Setup-tid:** 3-5 dagar
- **Professionell:** Som verkliga production systems

### **Implementation roadmap:**
1. **Fas 1:** Enkel pipeline med Cloud Functions + Cloud Run
2. **Fas 2:** Expansion till fler spel (1000+)
3. **Fas 3:** Cloud Composer + Vertex AI (valfritt)

### **NÃ¤sta steg:**
- âœ… Aktivera GCP APIs
- âœ… Deploya Cloud Function fÃ¶r data collection
- âœ… SÃ¤tt upp BigQuery dataset
- âœ… Deploya Cloud Run services
- âœ… Testa hela pipeline

---

*Detta dokument ska uppdateras kontinuerligt under projektets gÃ¥ng fÃ¶r att reflektera nuvarande status, lÃ¤rdomar och Ã¤ndringar i planeringen.*
