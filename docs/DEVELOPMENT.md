# IGDB Spelrekommendationssystem - Utvecklingsguide

## üéØ **Projektm√•l**

**Huvudm√•l:** Bygga ett komplett spelrekommendationssystem med IGDB API som datak√§lla, implementerat som en fullst√§ndig data pipeline i Google Cloud Platform.

**Slutprodukt:** En webbapplikation d√§r anv√§ndare kan skriva in spel-titlar och f√• rekommendationer p√• liknande spel baserat p√• ML-algoritmer.

---

## üèóÔ∏è **Teknisk Arkitektur**

### **Data Pipeline (End-to-End)**
```
IGDB API ‚Üí Airflow ‚Üí Cloud Storage ‚Üí BigQuery ‚Üí dbt ‚Üí ML Processing ‚Üí FastAPI ‚Üí Next.js Frontend
```

**Airflow Orchestration:**
- **Data Collection:** IGDB API ‚Üí Local JSON files
- **Storage Upload:** Local files ‚Üí Google Cloud Storage
- **BigQuery Load:** GCS ‚Üí BigQuery raw tables
- **dbt Transformations:** Raw data ‚Üí ML-ready features
- **ML Training:** Transformed data ‚Üí Trained models

### **Teknisk Stack**
- **Backend:** Python, FastAPI, IGDB API
- **Data Processing:** BigQuery, dbt (data build tool)
- **Data Orchestration:** Apache Airflow 3.0 ‚≠ê **IMPLEMENTERAT**
- **ML:** scikit-learn, pandas, numpy
- **Storage:** Google Cloud Storage ‚≠ê **IMPLEMENTERAT**
- **Frontend:** Next.js 14, TypeScript, Tailwind CSS, shadcn/ui
- **Cloud:** Google Cloud Platform (GCP)
- **CI/CD:** GitHub Actions
- **Containerization:** Docker

---

## üìä **Data K√§lla: IGDB API**

### **Tillg√§nglig Data (15+ Datatyper)**
- **Spel (Games):** namn, beskrivning, storyline, betyg, releasedatum, genrer, plattformar, teman
- **Kategorisering:** genrer, plattformar, teman, spelmoder, perspektiv
- **F√∂retag:** utvecklare, utgivare, beskrivningar, l√§nder
- **Media:** covers, screenshots, videos, websites
- **Tidsdata:** release dates, timestamps

### **Data Volym & Kvalitet**
- **~500,000+ spel** i databasen
- **~50+ genrer** och **~100+ teman**
- **~200+ plattformar** (alla tider)
- **~10,000+ f√∂retag** (utvecklare/utgivare)
- **Miljontals bilder** (covers, screenshots)
- **H√∂g datakvalitet:** Strukturerad, verifierad, historisk data

### **API Begr√§nsningar**
- **Rate limit:** ~30 requests per minute
- **Gratis tier:** Bra f√∂r testning och sm√• projekt
- **Autentisering:** OAuth2 via Twitch Developer Portal

---

## ü§ñ **Machine Learning Approach**

### **Rekommendationssystem (Huvudfokus)**
**M√•l:** Rekommendera liknande spel baserat p√• anv√§ndarinput

**Algoritmer:**
1. **Content-Based Filtering**
   - Analysera spel-genres, teman, plattformar
   - Ber√§kna similarity scores
   - Rekommendera spel med liknande attribut

2. **Collaborative Filtering** (om anv√§ndardata finns)
   - "Anv√§ndare som gillade X gillade ocks√• Y"
   - Anv√§nda rating-data f√∂r rekommendationer

3. **Hybrid Approach**
   - Kombinera content-based och collaborative
   - Viktning baserat p√• tillg√§nglig data

### **Feature Engineering**
```python
# Spel-attribut f√∂r ML:
- Genres (one-hot encoding)
- Themes (one-hot encoding)
- Platforms (one-hot encoding)
- Game modes (one-hot encoding)
- Player perspectives (one-hot encoding)
- Release year (numerical)
- Rating scores (numerical)
- Text features (TF-IDF p√• summaries)
```

---

## üé® **Anv√§ndarupplevelse**

### **Frontend (Next.js + shadcn/ui)**
**Huvudfunktioner:**
- **S√∂kf√§lt:** Anv√§ndare skriver in spel-titlar
- **Rekommendationer:** Visar liknande spel med:
  - Spel-titel och beskrivning
  - Cover-bild
  - Genres och teman
  - Rating och releasedatum
  - Likhetsscore
- **Filtrering:** Filtrera p√• genre, plattform, √•r
- **Responsiv design:** Fungerar p√• desktop och mobil

### **API Endpoints (FastAPI)**
```python
# Huvudendpoints:
GET /api/games/search?query={game_name}
GET /api/games/recommendations?game_id={id}&limit={n}
GET /api/games/{id}
GET /api/genres
GET /api/platforms
POST /api/recommendations/batch  # F√∂r flera spel samtidigt
```

---

## üöÄ **Utvecklingsfaser**

### **Fas 1: Frontend-First Prototyping** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Visuell feedback och iterativ utveckling

**Uppgifter:**
- [x] Skapa projektstruktur enligt best practice
- [x] Migrera Isaks IGDB API kod till `src/api/`
- [x] Utveckla data collection script
- [x] Bygg data preprocessing pipeline
- [x] **Frontend setup** med Next.js + shadcn/ui
- [x] **Data visualization** - visa testdata i tables/charts
- [x] **Budget tracking** dashboard f√∂r GCP credits
- [x] **Basic API endpoints** f√∂r data access
- [x] **GCP Integration** - budget monitoring med verklig data

### **Fas 2: Local-First ML Development** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Bygga robust rekommendationsmotor lokalt innan cloud scaling

**Strategi:** "Progressive Local-First" - utveckla och testa allt lokalt f√∂rst

**Uppgifter:**
- [x] **Data Collection (1,000+ spel)** - samla tillr√§ckligt med data lokalt
- [x] **Progressive feature engineering** - b√∂rja med core features (genres, themes)
- [x] **Local model training** p√• MacBook med scikit-learn
- [x] **Manual evaluation system** - "Ser dessa rekommendationer rimliga ut?"
- [x] **Frontend integration** - s√∂k + rekommendationer i UI
- [x] **Model comparison** - testa olika algoritmer visuellt
- [x] **Performance optimization** f√∂r lokala constraints
- [x] **Data quality validation** med visuell feedback

### **Fas 3: Docker & CI/CD Integration** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Containerisering och CI/CD-pipeline f√∂r skalning till molnet

**Uppgifter:**
- [x] **GCP budget tracking** - real-time cost monitoring
- [x] **Docker containerization** - Frontend + Backend + PostgreSQL
- [x] **TypeScript/ESLint fixes** - Clean builds utan fel
- [x] **Lokal Docker-testning** - Alla services fungerar perfekt
- [x] **GitHub Actions CI/CD** - Simple CI pipeline implementerad och fungerar
- [x] **GitHub CLI Integration** - Direkt workflow-√∂vervakning fr√•n terminal
- [x] **Python Code Quality** - Black, flake8, isort automation
- [x] **Pre-commit Hooks** - Lokal kodkvalitet f√∂re commit
- [x] **Status Badges** - Real-time CI/CD status i README
- [x] **Frontend Component Fixes** - TypeScript path mapping fixade, Docker build fungerar

### **Fas 4: Airflow Data Pipeline** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Automatiserad data pipeline fr√•n IGDB API till ML-modeller

**Uppgifter:**
- [x] **Airflow 3.0 Installation** - Apache Airflow med Google providers
- [x] **Cloud Storage Setup** - GCS buckets f√∂r raw och processed data
- [x] **Airflow DAG Development** - Komplett pipeline DAG implementerad
- [x] **Data Collection Task** - IGDB API ‚Üí Local JSON files
- [x] **Storage Upload Task** - Local files ‚Üí Google Cloud Storage
- [x] **BigQuery Load Task** - GCS ‚Üí BigQuery raw tables
- [x] **dbt Integration Task** - Raw data ‚Üí ML-ready features
- [x] **ML Training Task** - Transformed data ‚Üí Trained models
- [x] **Test DAG** - Verifiering av alla komponenter
- [x] **GCP Authentication** - Service account integration
- [x] **Airflow Configuration** - JWT secrets och s√§kra nycklar
- [x] **Web UI Access** - http://localhost:8080 fungerar perfekt
- [x] **Documentation** - Komplett setup guide i docs/AIRFLOW_SETUP.md

### **Fas 4: Airflow Data Pipeline** ‚≠ê **KLAR** ‚úÖ
**M√•l:** Automatiserad data pipeline fr√•n IGDB API till ML-modeller

**Implementation:**
- **Airflow 3.0** - Apache Airflow med Google providers
- **Cloud Storage** - GCS buckets f√∂r raw och processed data
- **DAG Development** - Komplett pipeline med 5 tasks
- **GCP Integration** - Service account authentication
- **Web UI** - http://localhost:8080 f√∂r monitoring
- **Documentation** - Komplett setup guide i docs/AIRFLOW_SETUP.md

**Pipeline Tasks:**
1. **collect_igdb_data** - IGDB API ‚Üí Local JSON files
2. **upload_to_gcs** - Local files ‚Üí Google Cloud Storage
3. **load_to_bigquery** - GCS ‚Üí BigQuery raw tables
4. **run_dbt_transformations** - Raw data ‚Üí ML-ready features
5. **train_ml_models** - Transformed data ‚Üí Trained models

### **Fas 5: Hybrid GCP Learning & Scaling** ‚≠ê **N√ÑSTA** üéØ
**Strategi:** "GCP Learning ‚Üí Local Scaling ‚Üí Cloud Production"

**M√•l:** L√§r dig GCP-tj√§nsterna med v√•r data, skala lokalt, sedan deploya till molnet

#### **Fas 4A: GCP Learning (1-2 dagar)** ‚úÖ **KLAR**
**Syfte:** L√§r dig BigQuery, dbt, Airflow, Vertex AI med v√•ra 100 spel

**Steg 1: BigQuery Setup** ‚úÖ **KLAR**
- [x] **Skapa BigQuery dataset** `igdb_games` (exalted-tempo-471613-e2)
- [x] **Ladda upp v√•ra 100 spel** fr√•n lokal CSV till BigQuery (games_raw tabell)
- [x] **Testa SQL queries** p√• speldata i BigQuery (83 kolumner, 100 spel)
- [x] **Skapa views** f√∂r genres, platforms, themes (genre_analysis, platform_analysis)

**Steg 2: dbt Project Setup** ‚úÖ **KLAR**
- [x] **Skapa dbt project** f√∂r data transformation (`dbt_igdb_project/igdb_models`)
- [x] **Definiera models** f√∂r games, genres, platforms (stg_games, game_recommendations)
- [x] **Testa transformations** lokalt med dbt (100 spel transformerade)
- [x] **Deploy till BigQuery** med dbt (US region, igdb_games dataset)

**Steg 3: Airflow DAG Setup**
- [ ] **Skapa Airflow DAG** f√∂r data pipeline
- [ ] **Definiera tasks** f√∂r data collection, transformation, ML
- [ ] **Testa DAG** lokalt med Airflow
- [ ] **Deploy till Cloud Composer** (eller lokal Airflow)

**Steg 4: Vertex AI Learning**
- [ ] **Skapa Vertex AI notebook** f√∂r ML experimentation
- [ ] **Testa ML tr√§ning** med v√•ra 100 spel i Vertex AI
- [ ] **J√§mf√∂r prestanda** med lokal tr√§ning
- [ ] **L√§r dig AutoML** f√∂r automatisk modelltr√§ning

---

## üõ†Ô∏è **Detaljerad Implementation Guide - Fas 4A**

### **Steg 1: BigQuery Setup (2-3 timmar)**

#### **1.1 Aktivera BigQuery API**
```bash
# Aktivera venv f√∂rst
source venv/bin/activate

# Aktivera BigQuery API
gcloud services enable bigquery.googleapis.com

# Verifiera att API √§r aktiverat
gcloud services list --enabled --filter="name:bigquery"
```

#### **1.2 Skapa BigQuery Dataset**
```bash
# Skapa dataset f√∂r v√•r data
bq mk --dataset --location=EU --description="IGDB Game Data for ML Pipeline" \
  exalted-tempo-471613-e2:igdb_game_data

# Verifiera att dataset skapades
bq ls exalted-tempo-471613-e2:igdb_game_data
```

#### **1.3 Ladda upp v√•ra 100 spel**
```bash
# Konvertera v√•r CSV till BigQuery format
python -c "
import pandas as pd
import json

# L√§s v√•r senaste processed data
df = pd.read_csv('data/processed/games_20250910_194944.csv')
print(f'Laddar {len(df)} spel till BigQuery...')

# Spara som JSON f√∂r BigQuery
df.to_json('games_for_bigquery.json', orient='records', lines=True)
print('Data konverterad till JSON format')
"

# Ladda upp till BigQuery
bq load --source_format=NEWLINE_DELIMITED_JSON \
  --autodetect \
  exalted-tempo-471613-e2:igdb_game_data.games \
  games_for_bigquery.json

# Verifiera att data laddades
bq query --use_legacy_sql=false "
SELECT COUNT(*) as total_games, 
       COUNT(DISTINCT genre_id) as unique_genres,
       COUNT(DISTINCT platform_id) as unique_platforms
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
"
```

#### **1.4 Skapa Views f√∂r Analysis**
```bash
# Skapa view f√∂r genre analysis
bq query --use_legacy_sql=false "
CREATE OR REPLACE VIEW \`exalted-tempo-471613-e2.igdb_game_data.genre_analysis\` AS
SELECT 
  genre_id,
  COUNT(*) as game_count,
  AVG(rating) as avg_rating,
  MIN(release_year) as earliest_year,
  MAX(release_year) as latest_year
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
WHERE genre_id IS NOT NULL
GROUP BY genre_id
ORDER BY game_count DESC
"

# Skapa view f√∂r platform analysis
bq query --use_legacy_sql=false "
CREATE OR REPLACE VIEW \`exalted-tempo-471613-e2.igdb_game_data.platform_analysis\` AS
SELECT 
  platform_id,
  COUNT(*) as game_count,
  AVG(rating) as avg_rating,
  MIN(release_year) as earliest_year,
  MAX(release_year) as latest_year
FROM \`exalted-tempo-471613-e2.igdb_game_data.games\`
WHERE platform_id IS NOT NULL
GROUP BY platform_id
ORDER BY game_count DESC
"
```

### **Steg 2: dbt Project Setup (2-3 timmar)** ‚úÖ **KLAR**

#### **2.1 Installera dbt** ‚úÖ **KLAR**
```bash
# Installera dbt med BigQuery support
pip install dbt-bigquery

# Verifiera installation
dbt --version
```

#### **2.2 Skapa dbt Project** ‚úÖ **KLAR**
```bash
# Skapa dbt project
mkdir dbt_igdb_project
cd dbt_igdb_project

# Initiera dbt project
dbt init igdb_models

# Konfigurera profiles.yml f√∂r BigQuery
# Dataset: igdb_games (US region)
# Project: exalted-tempo-471613-e2
# Authentication: Service Account
```

#### **2.3 Skapa dbt Models** ‚úÖ **KLAR**

**Staging Model (`stg_games.sql`):**
- Rensar och standardiserar raw games data
- Data quality checks och derived fields
- Rating categories (Excellent, Good, Average, etc.)
- Era categories (Recent, Modern, Classic, Retro)

**Marts Model (`game_recommendations.sql`):**
- ML-optimerad tabell f√∂r rekommendationer
- Feature vectors f√∂r genre similarity
- Imputed values f√∂r missing data
- Processed timestamps f√∂r data freshness

#### **2.4 dbt Configuration** ‚úÖ **KLAR**
```yaml
# dbt_project.yml
models:
  igdb_models:
    staging:
      +materialized: view
    marts:
      +materialized: table
```

#### **2.5 Data Tests** ‚úÖ **KLAR**
- Unique constraints p√• game IDs
- Not-null validations p√• kritiska f√§lt
- Source data quality tests
- 12/13 tests passerade (1 example test misslyckades)

#### **2.6 Dataset Configuration** ‚úÖ **KLAR**
**BigQuery Datasets:**
- `igdb_games` (US region) - ‚úÖ **AKTIV** - V√•r data och dbt models
- `igdb_game_data` (EU region) - ‚ö†Ô∏è **OANV√ÑND** - Skapad tidigare, kan tas bort

**Region Val:**
- **US region** vald f√∂r b√§ttre prestanda med GCP-tj√§nster
- **EU region** hade kr√§vt extra konfiguration f√∂r andra GCP-tj√§nster
- **Rekommendation:** Beh√•ll US region f√∂r konsistens

#### **2.3 Konfigurera dbt Profile**
```bash
# Skapa profiles.yml
mkdir -p ~/.dbt
cat > ~/.dbt/profiles.yml << 'EOF'
igdb_bigquery:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      keyfile: /path/to/your/service-account-key.json
      project: exalted-tempo-471613-e2
      dataset: igdb_game_data
      location: EU
      threads: 4
      timeout_seconds: 300
EOF
```

#### **2.4 Skapa dbt Models**
```bash
# Skapa staging models
mkdir -p models/staging
cat > models/staging/stg_games.sql << 'EOF'
-- Staging model f√∂r games data
SELECT 
  id,
  name,
  summary,
  rating,
  release_year,
  genre_id,
  platform_id,
  theme_id,
  created_at,
  updated_at
FROM {{ source('raw', 'games') }}
WHERE name IS NOT NULL
EOF

# Skapa marts models
mkdir -p models/marts
cat > models/marts/game_recommendations.sql << 'EOF'
-- Mart model f√∂r game recommendations
SELECT 
  g.id,
  g.name,
  g.summary,
  g.rating,
  g.release_year,
  g.genre_id,
  g.platform_id,
  g.theme_id,
  CASE 
    WHEN g.rating >= 80 THEN 'Excellent'
    WHEN g.rating >= 70 THEN 'Good'
    WHEN g.rating >= 60 THEN 'Average'
    ELSE 'Below Average'
  END as rating_category
FROM {{ ref('stg_games') }} g
WHERE g.rating IS NOT NULL
EOF
```

#### **2.5 Testa dbt**
```bash
# Testa dbt connection
dbt debug

# K√∂r dbt models
dbt run

# Testa dbt models
dbt test

# Generera dokumentation
dbt docs generate
dbt docs serve
```

### **Steg 3: Airflow DAG Setup (2-3 timmar)**

#### **3.1 Installera Airflow**
```bash
# Installera Airflow
pip install apache-airflow

# Initiera Airflow
airflow db init

# Skapa admin user
airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin
```

#### **3.2 Skapa Airflow DAG**
```bash
# Skapa DAGs mapp
mkdir -p ~/airflow/dags

# Skapa IGDB data pipeline DAG
cat > ~/airflow/dags/igdb_data_pipeline.py << 'EOF'
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'igdb-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 11),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'igdb_data_pipeline',
    default_args=default_args,
    description='IGDB Game Data Collection and Processing Pipeline',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

def collect_game_data():
    """Collect game data from IGDB API"""
    import subprocess
    subprocess.run(['python', 'collect_data.py', '--games-limit', '100'], cwd='/path/to/your/project')
    return "Data collection completed"

def process_data():
    """Process collected data"""
    import subprocess
    subprocess.run(['python', '-m', 'src.data_processing.etl_pipeline'], cwd='/path/to/your/project')
    return "Data processing completed"

def train_model():
    """Train ML model"""
    import subprocess
    subprocess.run(['python', '-m', 'src.models.train_recommender'], cwd='/path/to/your/project')
    return "Model training completed"

# Define tasks
collect_task = PythonOperator(
    task_id='collect_game_data',
    python_callable=collect_game_data,
    dag=dag,
)

process_task = PythonOperator(
    task_id='process_data',
    python_callable=process_data,
    dag=dag,
)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag,
)

# Define task dependencies
collect_task >> process_task >> train_task
EOF
```

#### **3.3 Starta Airflow**
```bash
# Starta Airflow webserver
airflow webserver --port 8080 &

# Starta Airflow scheduler
airflow scheduler &

# √ñppna Airflow UI
open http://localhost:8080
```

### **Steg 4: Vertex AI Learning (2-3 timmar)**

#### **4.1 Aktivera Vertex AI API**
```bash
# Aktivera Vertex AI API
gcloud services enable aiplatform.googleapis.com

# Verifiera att API √§r aktiverat
gcloud services list --enabled --filter="name:aiplatform"
```

#### **4.2 Skapa Vertex AI Notebook**
```bash
# Skapa Vertex AI notebook instance
gcloud ai notebooks instances create igdb-ml-notebook \
  --location=europe-west1 \
  --machine-type=e2-standard-4 \
  --vm-image-project=deeplearning-platform-release \
  --vm-image-family=tf2-2-8-cpu \
  --vm-image-name=tf2-2-8-cpu-20220119-170516

# √ñppna notebook
gcloud ai notebooks instances open igdb-ml-notebook --location=europe-west1
```

#### **4.3 Testa ML tr√§ning i Vertex AI**
```python
# Skapa notebook cell f√∂r ML tr√§ning
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from google.cloud import bigquery

# Ladda data fr√•n BigQuery
client = bigquery.Client()
query = """
SELECT id, name, summary, rating, genre_id, platform_id, theme_id
FROM `exalted-tempo-471613-e2.igdb_game_data.games`
WHERE summary IS NOT NULL
"""
df = client.query(query).to_dataframe()

# Tr√§na enkel content-based model
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(df['summary'].fillna(''))

# Ber√§kna similarity matrix
similarity_matrix = cosine_similarity(tfidf_matrix)

print(f"Tr√§nat model med {len(df)} spel")
print(f"Similarity matrix shape: {similarity_matrix.shape}")
```

---

## üöÄ **Skalningsstrategi: 100 ‚Üí 100,000+ Spel**

### **Kritiska Skalningsutmaningar & L√∂sningar**

#### **1. Data Collection Bottlenecks** üî• **KRITISKT**
**Problem:** IGDB API rate limit (30 req/min) blir flaskhals vid 100,000+ spel
**L√∂sningar:**
- **Batch Processing:** Samla data i chunks (1000 spel per batch)
- **Parallel Processing:** Anv√§nda Cloud Functions f√∂r parallell data collection
- **Caching Strategy:** Redis/Memcached f√∂r att undvika duplicerade API calls
- **Incremental Updates:** Bara samla nya/uppdaterade spel dagligen

#### **2. Storage & Processing Limits** üíæ **VIKTIGT**
**Problem:** Lokal storage och minne r√§cker inte f√∂r 100,000+ spel
**L√∂sningar:**
- **Cloud Storage:** GCS buckets f√∂r raw data (billigare √§n BigQuery)
- **Data Partitioning:** Partitionera data per √•r/genre f√∂r snabbare queries
- **Streaming Processing:** Cloud Dataflow f√∂r real-time data processing
- **Data Archiving:** Flytta gamla data till Coldline Storage

#### **3. ML Model Performance** ü§ñ **KRITISKT**
**Problem:** Cosine similarity p√• 100,000+ spel blir extremt l√•ngsam
**L√∂sningar:**
- **Vector Databases:** Pinecone/Weaviate f√∂r snabba similarity searches
- **Model Optimization:** Anv√§nda approximate nearest neighbors (ANN)
- **Feature Selection:** Reducera dimensions med PCA/t-SNE
- **Distributed Training:** Vertex AI f√∂r parallell modelltr√§ning

#### **4. API Response Times** ‚ö° **VIKTIGT**
**Problem:** Rekommendationer tar f√∂r l√•ng tid med stora datasets
**L√∂sningar:**
- **Precomputed Recommendations:** Ber√§kna rekommendationer i f√∂rv√§g
- **Caching Layer:** Redis f√∂r snabba API responses
- **CDN:** CloudFlare f√∂r statisk content
- **Database Indexing:** Optimera BigQuery queries med proper indexing

### **Best Practice Architecture f√∂r Skalning**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   IGDB API      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Cloud Functions ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Cloud Storage  ‚îÇ
‚îÇ  (Rate Limited) ‚îÇ    ‚îÇ  (Parallel Jobs) ‚îÇ    ‚îÇ   (Raw Data)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Vertex AI     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Airflow DAG    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   BigQuery      ‚îÇ
‚îÇ  (ML Training)  ‚îÇ    ‚îÇ (Orchestration)  ‚îÇ    ‚îÇ (Processed Data)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Cloud Run     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Redis Cache    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Vector DB     ‚îÇ
‚îÇ  (API Serving)  ‚îÇ    ‚îÇ (Fast Responses) ‚îÇ    ‚îÇ (Similarity)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Skalningsfaser**

#### **Fas 1: 100 ‚Üí 1,000 spel** (Nuvarande)
- ‚úÖ BigQuery setup klar
- üîÑ dbt project f√∂r data transformation
- üîÑ Airflow DAG f√∂r automation
- üîÑ Basic ML model optimization

#### **Fas 2: 1,000 ‚Üí 10,000 spel** (N√§sta vecka)
- üîÑ Cloud Storage integration
- üîÑ Parallel data collection
- üîÑ Model performance optimization
- üîÑ Caching implementation

#### **Fas 3: 10,000 ‚Üí 100,000+ spel** (Framtida)
- üîÑ Vector database integration
- üîÑ Distributed ML training
- üîÑ Advanced caching strategies
- üîÑ Production monitoring

---

## üéØ **N√§sta Steg efter Fas 4A**

N√§r du har genomf√∂rt Fas 4A kommer du att ha:
- ‚úÖ **BigQuery dataset** med v√•ra 100 spel
- ‚úÖ **dbt project** f√∂r data transformation
- ‚úÖ **Airflow DAG** f√∂r data pipeline
- ‚úÖ **Vertex AI notebook** f√∂r ML experimentation

**D√• √§r du redo f√∂r Fas 4B: Local Scaling** med 10,000+ spel!

#### **Fas 4B: Local Scaling (1-2 dagar)** üîÑ
**Syfte:** Skala upp till 10,000+ spel lokalt n√§r du f√∂rst√•r GCP

**Steg 1: Data Collection Scaling**
- [ ] **Samla 10,000+ spel** med `collect_data.py --games-limit 10000`
- [ ] **Optimera data collection** f√∂r st√∂rre volymer
- [ ] **Implementera batch processing** f√∂r effektivitet
- [ ] **Validera data quality** med st√∂rre dataset

**Steg 2: ML Algorithm Optimization**
- [ ] **Testa olika ML-algoritmer** med 10,000+ spel
- [ ] **Optimera feature engineering** f√∂r prestanda
- [ ] **Implementera model evaluation** med cross-validation
- [ ] **J√§mf√∂r algoritmer** (content-based vs collaborative)

**Steg 3: Performance Optimization**
- [ ] **Optimera Docker builds** f√∂r snabbare deployment
- [ ] **Implementera caching** f√∂r API responses
- [ ] **Optimera database queries** f√∂r prestanda
- [ ] **Testa load testing** med st√∂rre datam√§ngder

#### **Fas 4C: Cloud Production (1-2 dagar)** üöÄ
**Syfte:** Deploy komplett system till GCP n√§r allt fungerar lokalt

**Steg 1: Cloud Run Deployment**
- [ ] **Deploy frontend** till Cloud Run
- [ ] **Deploy backend** till Cloud Run
- [ ] **Konfigurera Cloud SQL** f√∂r PostgreSQL
- [ ] **Testa end-to-end** i molnet

**Steg 2: Data Pipeline Production**
- [ ] **Deploy Airflow DAG** till Cloud Composer
- [ ] **Konfigurera BigQuery** f√∂r production data
- [ ] **Deploy dbt models** f√∂r data transformation
- [ ] **Automatisera data pipeline** med Airflow

**Steg 3: ML Production**
- [ ] **Deploy ML model** till Vertex AI
- [ ] **Konfigurera model serving** med Cloud Run
- [ ] **Implementera model monitoring** med Vertex AI
- [ ] **Automatisera model retraining** med Airflow

**Steg 4: Production Monitoring**
- [ ] **S√§tt upp Cloud Monitoring** f√∂r system health
- [ ] **Konfigurera budget alerts** f√∂r kostnadskontroll
- [ ] **Implementera logging** med Cloud Logging
- [ ] **S√§tt upp error tracking** med Cloud Error Reporting

### **Fas 5: Advanced ML & Production** ‚≠ê **FRAMTIDA** üîÆ
**M√•l:** Production-ready system med avancerade funktioner

**Uppgifter:**
- [ ] **Advanced features** - text analysis, visual similarity
- [ ] **A/B testing** framework med frontend integration
- [ ] **User feedback** system f√∂r continuous improvement
- [ ] **Real-time rekommendationer** med caching
- [ ] **Performance monitoring** med budget tracking
- [ ] **CI/CD pipeline** med automated testing
- [ ] **Documentation** och presentation f√∂r kursen

---

## üìà **Success Metrics**

### **Tekniska Metrics**
- **API Response Time:** < 200ms f√∂r rekommendationer
- **Data Freshness:** Daglig uppdatering av speldata
- **System Uptime:** > 99.5%
- **Model Accuracy:** > 80% relevanta rekommendationer

### **Anv√§ndarupplevelse Metrics**
- **Search Success Rate:** > 90% hittar s√∂kta spel
- **Recommendation Relevance:** Anv√§ndarfeedback > 4/5
- **Page Load Time:** < 2 sekunder
- **Mobile Responsiveness:** Fungerar p√• alla enheter

### **Business Metrics**
- **Data Pipeline Efficiency:** < 1 timme f√∂r fullst√§ndig datauppdatering
- **Cost Optimization:** < $100/m√•nad i GCP-kostnader (med budget tracking)
- **Scalability:** St√∂der 1000+ samtidiga anv√§ndare
- **Budget Utilization:** < 80% av tillg√§ngliga GCP credits
- **Development Velocity:** Visuell feedback inom 1 dag f√∂r varje feature

---

## üéØ **N√§sta Steg - Hybrid GCP Strategy**

### **Omedelbara √•tg√§rder (Idag):**
1. **Fas 4A: GCP Learning** - B√∂rja med BigQuery setup
2. **Ladda upp v√•ra 100 spel** till BigQuery f√∂r att l√§ra sig
3. **Skapa dbt project** f√∂r data transformation
4. **Testa Airflow DAG** lokalt

### **Denna vecka:**
- **Fas 4A:** GCP Learning (BigQuery, dbt, Airflow, Vertex AI)
- **Fas 4B:** Local Scaling (10,000+ spel lokalt)
- **Fas 4C:** Cloud Production (deploy till GCP)

### **Kommande veckor:**
- **Vecka 4:** Advanced ML och production deployment
- **Vecka 5:** Kurs presentation och dokumentation

---

## üìù **Projektstatus**

**Senast uppdaterad:** 2025-01-11
**Nuvarande fas:** Fas 4A - GCP Learning (‚úÖ BigQuery Klar) + Docker & CI/CD Integration (‚úÖ Klar) + Local-First ML Development (‚úÖ Klar) + Frontend Integration (‚úÖ Klar) + Data Quality Dashboard (‚úÖ Klar) + GitHub Actions CI/CD (‚úÖ Klar)
**N√§sta milestone:** dbt Project Setup och Data Pipeline Architecture f√∂r skalning
**Gruppmedlemmar:** Viktoria, Isak & Johan
**Teknisk stack:** Python, Next.js, shadcn/ui, Docker, GCP, IGDB API
**Budget:** AI24S-Data-Engineering-IGDB (kr100.00/m√•nad) + $300 GCP credits
**GCP Project:** IGDB-ML-Pipeline (exalted-tempo-471613-e2)
**Strategi:** Hybrid GCP Learning ‚Üí Local Scaling ‚Üí Cloud Production
**Status:** Komplett fungerande system med 100 spel, ML-rekommendationer, data quality dashboard och Docker containerization. Redo f√∂r GCP learning och scaling.

---

*Detta dokument ska uppdateras kontinuerligt under projektets g√•ng f√∂r att reflektera nuvarande status, l√§rdomar och √§ndringar i planeringen.*
